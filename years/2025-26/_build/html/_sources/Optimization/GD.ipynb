{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "We have seen that the **training phase** of a Machine Learning model can be expressed as an **optimization problem**: given a loss function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta) = \\frac{1}{N}\\sum_{i=1}^N \\ell(f_\\Theta(x^{(i)}), y^{(i)}),\n",
    "$$\n",
    "\n",
    "our goal is to find\n",
    "\n",
    "$$\n",
    "\\Theta^* = \\arg\\min_\\Theta \\, \\mathcal{L}(\\Theta).\n",
    "$$\n",
    "\n",
    "\n",
    "For simple cases (like linear regression with one parameter), this minimization problem may admit a closed-form solution. In general, however, the loss function is **non-linear, high-dimensional, and often non-convex**, making analytic solutions impossible.  \n",
    "We therefore rely on **iterative optimization algorithms**, the most fundamental of which is **Gradient Descent**.\n",
    "\n",
    "## First-Order (and Second-Order) Optimality Conditions\n",
    "\n",
    "For unconstrained, differentiable problems $ \\min_\\Theta \\mathcal{L}(\\Theta) $:\n",
    "\n",
    "- **First-Order Necessary Condition**: any local minimizer $\\Theta^*$ satisfies  \n",
    "  \n",
    "  $$\n",
    "  \\nabla_\\Theta \\mathcal{L}(\\Theta^*) = 0.\n",
    "  $$\n",
    "\n",
    "  Points with zero gradient are called **stationary points** (they can be minima, maxima, or saddle points).\n",
    "\n",
    "- **Second-Order Test** (when $\\mathcal{L}$ is twice differentiable):  \n",
    "  Let $H(\\Theta) = \\nabla^2_\\Theta \\mathcal{L}(\\Theta)$ be the Hessian.  \n",
    "  - If $H(\\Theta^*)$ is **positive definite**, $\\Theta^*$ is a **strict local minimum**.  \n",
    "  - If $H(\\Theta^*)$ is **indefinite**, $\\Theta^*$ is a **saddle point**.  \n",
    "  - If $\\mathcal{L}$ is **convex**, then **any** stationary point is a **global minimum**. If it is **strictly convex**, the global minimizer is **unique**.\n",
    "\n",
    "![](./fig/minmaxsaddle.png)\n",
    "\n",
    "## The Gradient Descent Algorithm\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The gradient of a differentiable function $\\nabla_\\Theta \\mathcal{L}(\\Theta)$ points in the direction of the **steepest ascent**. If we want to minimize the function, we must move in the **opposite direction**. This leads to the update rule:\n",
    "\n",
    "$$\n",
    "\\Theta^{(k+1)} = \\Theta^{(k)} - \\eta_k \\, \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}),\n",
    "$$\n",
    "\n",
    "where $\\eta_k > 0$, $k = 1, \\dots, \\texttt{maxit}$, is the **learning rate** (step size).\n",
    "\n",
    "Therefore, the resulting algorithm reads:\n",
    "\n",
    "**Input**: initial parameters $\\Theta^{(0)}$, learning rate schedule $\\{ \\eta_k \\}_k$, stopping criterion.  \n",
    "**Repeat** until the stopping criterion is satisfied:\n",
    "1. Compute the gradient $\\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)})$.  \n",
    "2. Update parameters:\n",
    "   \n",
    "   $$\n",
    "   \\Theta^{(k+1)} = \\Theta^{(k)} - \\eta_k \\, \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}).\n",
    "   $$\n",
    "\n",
    "```{note}\n",
    "The gradient descent algorithm is an example of descent methods:\n",
    "\n",
    "$$\n",
    "\\Theta^{(k+1)} = \\Theta^{(k)} + \\eta_k p_k, \\quad k = 0, 1, \\dots, \\texttt{maxit},\n",
    "$$\n",
    "\n",
    "where the descent direction $p_k$ has to be chosen such that for any $k \\in \\mathbb{N}$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)})^T p_k \\leq 0,\n",
    "$$\n",
    "\n",
    "to assure convergence to a stationary point. Since for GD, $p_k = - \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)})$, then the condition above is always satisfied:\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)})^T p_k = - \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)})^T \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}) = - || \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}) ||_2^2 \\leq 0.\n",
    "$$\n",
    "\n",
    "Therefore, GD algorithm **always** converge to a stationary point in the limit of infinite iterations.\n",
    "```\n",
    "\n",
    "```{warning}\n",
    "Do **not** confuse this full-batch Gradient Descent with **Stochastic Gradient Descent** (SGD), which will be introduced in the next chapter.\n",
    "```\n",
    "\n",
    "## Selection of parameters\n",
    "\n",
    "### Choice the initial iterate\n",
    "\n",
    "The Gradient Descent (GD) algorithm, require the user to input an initial iterate $\\Theta^{(0)} \\in \\mathbb{R}^n$. Theoretically, since GD has a _global convergence_ proprerty, for any $\\Theta^{(0)}$ it will always converge to a **stationary point** of $\\mathcal{L}(\\Theta)$, i.e. to a point such that $\\nabla_\\Theta \\mathcal{L}(\\Theta) = 0$.\n",
    "\n",
    "If $\\mathcal{L}(\\Theta)$ is convex, then every stationary point is a (global) minimum of $\\mathcal{L}(\\Theta)$, implying that the choice of $\\Theta^{(0)}$ is not really important, and we can always set $\\Theta^{(0)} = 0$. On the other side, when $\\mathcal{L}(\\Theta)$ is not convex, we have to choose $\\Theta^{(0)}$ such that it is as close as possible to the _right_ stationary point, to increase the chances of getting to that. If an estimate of the correct minimum point is not available, we will just consider $\\Theta^{(0)} = 0$ to get to a general local minima.\n",
    "\n",
    "### Step-Size Selection (Learning Rate)\n",
    "\n",
    "The choice of $\\{ \\eta_k \\}_k$ critically affects convergence.\n",
    "\n",
    "- **Constant step size**: given a value $\\eta$, set $\\eta_k = \\eta$ $\\forall k = 1, \\dots, \\texttt{maxit}$. Simple, but problem-scale dependent. In particular:\n",
    "  - If $\\eta$ is **too small**: GD does not converge as the iterates stucks before reaching convergence;\n",
    "  - If $\\eta$ is **too large**: The iterates bounce back and forth around the minima, unable to reach it;\n",
    "  - If $\\eta$ is **just right**: The GD method converge to a stationary point, with a speed proportional to how large $\\eta$ is.\n",
    "  \n",
    "![](./fig/right_alpha.png)\n",
    "\n",
    "- **Backtracking line search**: for any $k \\in 1, \\dots, \\texttt{maxit}$, set $\\eta_k = \\bar{\\eta}$ and shrink by $\\beta \\in (0,1)$ until the **Armijo** condition holds:\n",
    "  \n",
    "  $$\n",
    "  \\mathcal{L}(\\Theta^{(k)} - \\eta g^{(k)}) \\;\\le\\; \\mathcal{L}(\\Theta^{(k)}) - c\\,\\eta_k\\,\\|g^{(k)}\\|^2,\n",
    "  \\qquad g^{(k)}=\\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}),\\; c\\in(0,1).\n",
    "  $$\n",
    "\n",
    "**Backtracking:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def backtracking(L, grad_L, theta, eta0=1.0, beta=0.5, c=1e-4):\n",
    "    \"\"\"\n",
    "    Return a step size eta that satisfies the Armijo condition:\n",
    "        L(theta - eta*g) <= L(theta) - c * eta * ||g||^2\n",
    "    Inputs:\n",
    "      - L:      R^n -> R\n",
    "      - grad_L: R^n -> R^d\n",
    "      - theta:  current point (np.ndarray)\n",
    "      - eta0:   initial step size\n",
    "      - beta:   shrinking factor in (0,1)\n",
    "      - c:      Armijo constant in (0,1)\n",
    "    \"\"\"\n",
    "    eta = eta0\n",
    "    g = grad_L(theta)\n",
    "    g_norm2 = np.dot(g, g)\n",
    "    while L(theta - eta * g) > L(theta) - c * eta * g_norm2:\n",
    "        eta *= beta\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Criteria\n",
    "\n",
    "In practice, we cannot iterate forever. Common **stopping criteria** include:\n",
    "- The **norm of the gradient** is small:\n",
    "  \n",
    "  $$\n",
    "  \\|\\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)})\\| < \\texttt{tol}_\\mathcal{L},\n",
    "  $$\n",
    "\n",
    "  meaning we are close to a stationary point.  \n",
    "- The **change in loss** is negligible:\n",
    "  \n",
    "  $$\n",
    "  |\\mathcal{L}(\\Theta^{(k+1)}) - \\mathcal{L}(\\Theta^{(k)})| < \\texttt{tol}_{ES}.\n",
    "  $$\n",
    "\n",
    "  named **early stopping** In ML practice (usually estimated on the validation set).\n",
    "\n",
    "- The **parameter update** is small:\n",
    "  \n",
    "  $$\n",
    "  \\|\\Theta^{(k+1)} - \\Theta^{(k)}\\| < \\texttt{tol}_\\Theta.\n",
    "  $$\n",
    "\n",
    "- A **maximum number of iterations** $\\texttt{maxit}$ is reached.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ill-Conditioning and Geometry\n",
    "\n",
    "Ill-conditioning manifests as elongated level sets and **zig-zagging** GD trajectories, which makes hard for GD to reach convergence.\n",
    "\n",
    "- Given $0<\\lambda_1\\ll\\lambda_2$, consider the loss function:\n",
    "  \n",
    "  $$\n",
    "  \\mathcal{L}(\\Theta) = \\frac{1}{2} \\lambda_1 \\Theta_1^2 + \\lambda_2 \\Theta_2^2,\n",
    "  $$\n",
    "\n",
    "The level sets are ellipses whose axes scale with $\\lambda_1^{-1/2}$ and $\\lambda_2^{-1/2}$. When the ratio $\\lambda_2/\\lambda_1$ is large, Gradient Descent with a fixed $\\eta$ progresses slowly and “zig-zags” across narrow valleys.\n",
    "\n",
    "> **Exercise (condition number intuition; left-multiplication):**  \n",
    "> Sketch the level sets of $\\mathcal{L}(\\Theta)=\\tfrac{1}{2}\\,\\Theta^\\top A\\,\\Theta$ for $A=\\mathrm{diag}(\\lambda_1,\\lambda_2)$.  \n",
    "\n",
    "**Level-set plot helper:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def quad_levelsets(A, xlim=(-3,3), ylim=(-3,3), ngrid=400, ncontours=12, title=None):\n",
    "    xs = np.linspace(xlim[0], xlim[1], ngrid)\n",
    "    ys = np.linspace(ylim[0], ylim[1], ngrid)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    Z = 0.5*(A[0,0]*X**2 + 2*A[0,1]*X*Y + A[1,1]*Y**2)  # theta^T A theta, left-multiplied convention\n",
    "    cs = plt.contour(X, Y, Z, levels=ncontours)\n",
    "    plt.clabel(cs, inline=True, fontsize=8)\n",
    "    plt.axhline(0, lw=0.5, color='k')\n",
    "    plt.axvline(0, lw=0.5, color='k')\n",
    "    plt.gca().set_aspect('equal', 'box')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel(r'$\\theta_1$')\n",
    "    plt.ylabel(r'$\\theta_2$')\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.show()\n",
    "\n",
    "# Example: ill-conditioned ellipse\n",
    "A = np.diag([1.0, 25.0])  # lambda1 << lambda2\n",
    "quad_levelsets(A, title='Level sets of 0.5 * theta^T A theta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Pure Gradient Descent\n",
    "\n",
    "Despite its usefulness, Gradient Descent has several drawbacks:\n",
    "\n",
    "1. **Computationally expensive**: computing the gradient requires scanning the entire dataset at every step.  \n",
    "2. **Learning rate sensitivity**: too small = slow convergence, too large = divergence/oscillations.  \n",
    "3. **Local minima, saddle points, and plateaus** in non-convex landscapes.  \n",
    "4. **Ill-conditioning** causes slow progress and zig-zagging.\n",
    "\n",
    "### Motivation for Stochastic Gradient Descent\n",
    "\n",
    "These limitations motivate more practical algorithms. In particular, instead of computing the gradient using the *entire dataset* at every iteration, one can approximate it using a **subset of the data (mini-batches)**. This gives rise to **Stochastic Gradient Descent (SGD)**, the subject of the next chapter.  \n",
    "\n",
    "SGD addresses the scalability issue and often improves generalization, at the cost of introducing stochasticity in the updates.\n",
    "\n",
    "In summary, **Gradient Descent** provides the foundational principles of optimization in Machine Learning:  \n",
    "- It is an iterative method for minimizing loss based on **negative gradients**.  \n",
    "- Its behavior hinges on **convexity/strict convexity**, **smoothness**, **step-size selection** (constant, exact line search, or **backtracking/Armijo**), and **stopping criteria**.  \n",
    "- Its practical limitations naturally lead to **Stochastic Gradient Descent**, which we study next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
