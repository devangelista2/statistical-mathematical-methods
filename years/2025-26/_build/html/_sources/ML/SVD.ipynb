{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Compression with Singular Value Decomposition (SVD)\n",
    "\n",
    "## Singular Value Decomposition of a Matrix\n",
    "In Data Analysis, it is often required to compress the data, either to make it more manageable or to be able to visualize the most important features, reducing redundancy. The two tasks, usually named **data compression** and **dimensionality reduction**, are mathematically equivalent and closely related to the concept of **Singular Value Decomposition (SVD)** of a matrix.\n",
    "\n",
    "Recall that:\n",
    "> An invertible matrix $A \\in \\mathbb{R}^{n \\times n}$ is said to be **orthogonal** if $A^T A = I$ or, equivalently, $A^T = A^{-1}$. \n",
    "\n",
    "Now, consider a given matrix $A \\in \\mathbb{R}^{m \\times n}$. It is known that it can be factorized into the product of three matrices,\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, while $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular matrix which is non-zero only on the diagonal. Such decomposition is named **Singular Value Decomposition (SVD)** of $A$.\n",
    "\n",
    "Of particular interest in our analysis are the values on the diagonal of $\\Sigma$, named **singular values** of $A$, and usually denoted as $\\sigma_1, \\dots, \\sigma_{\\min \\{ m, n \\}}$. In particular, it is known that the singular values:\n",
    "\n",
    "- are always greater or equal to 0, i.e. $\\sigma_i \\geq 0$, $\\forall i$;\n",
    "- are ordered in descending order, i.e. $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq 0$;\n",
    "- can be used to determine the rank of $A$, since it is equal to the number of singular values strictly greater than zero, i.e. if $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\sigma_r > 0$ and $\\sigma_{r+1} = 0$ for some index $r$, then $r = rk(A)$.\n",
    "\n",
    "A useful properties of the SVD of $A$ is that it can be used to compress the informations contained in $A$ itself. Indeed, note that the SVD decomposition allows to rewrite $A$ as the sum of simple matrices, i.e.\n",
    "\n",
    "$$\n",
    "    A = U \\Sigma V^T = \\sum_{i=1}^r \\sigma_i u_i v_i^T\n",
    "$$\n",
    "\n",
    "where $u_i$ and $v_i$ are the columns of $U$ and $V$, respectively. Each term $u_i v_i^T$ is a rank-1 matrix named **dyad**, and the $i$-th singular value $\\sigma_i$ represent the importance of the $i$-th dyad in the construction of $A$. In particular, the SVD decomposition allows to deconstruct $A$ into the sum of matrices with decreasing information content. \n",
    "\n",
    "The SVD decomposition can be used to compress the matrix $A$ by considering its $k$-rank approximation $A_k$, defined as\n",
    "\n",
    "$$\n",
    "    A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T.\n",
    "$$\n",
    "\n",
    "It has been already showed that the $k$-rank approximation of $A$ is the $k$-rank matrix that minimizes the distance (expressed in Frobenius norm) from $A$, i.e.\n",
    "\n",
    "$$\n",
    "    A_k = \\arg\\min_{M: rk(M) = k} || M - A ||_F.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD of a Matrix in Python\n",
    "The functions required to compute SVD decomposition of a matrix in Python are contained into the `numpy` package. In the following, we will consider the example matrix reported into the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing numpy\n",
    "import numpy as np\n",
    "\n",
    "# Consider an example matrix\n",
    "A = np.array(\n",
    "    [\n",
    "        [-1, -2, 0, 1, -2, -3],\n",
    "        [-1, -2, -3, -2, 0, -3],\n",
    "        [-1, -3, 1, 3, 2, -4],\n",
    "        [2, 1, -1, 0, -2, 3],\n",
    "        [0, -3, -1, 2, -1, -3],\n",
    "        [1, -3, 2, 6, 0, -2],\n",
    "        [-3, 1, 0, -4, 2, -2],\n",
    "        [-2, 2, -2, -6, -2, 0],\n",
    "        [-3, -1, 2, 0, 2, -4],\n",
    "        [2, -2, 0, 4, -1, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Measure the shape of A: which is the maximum rank?\n",
    "m, n = A.shape\n",
    "print(f\"The shape of A is: {(m, n)}.\")\n",
    "\n",
    "# Compute the SVD decomposition of A and check the shapes\n",
    "U, s, VT = np.linalg.svd(A, full_matrices=True)\n",
    "print(U.shape, s.shape, VT.shape)\n",
    "\n",
    "# Define the full matrix S\n",
    "S = np.zeros((m, n))\n",
    "S[:n, :n] = np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we computed the SVD of `A` through the function `np.linalg.svd()`, that takes as input a matrix and returns a triplet `U, s, VT`, representing the matrices $U$, $V^T$ and a **vectorized** version of $\\Sigma$ that only contains the diagonal (to save memory!).\n",
    "\n",
    "Note that, in some situations, it can be useful to compute the full matrix $\\Sigma$, as we did at the bottom of the above code snippet. \n",
    "\n",
    "```{warning}\n",
    "If the full matrix $\\Sigma$ can be avoided, do not construct it explicitely.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Verify that the above algorithm works as expected, by proving that $ A \\approx U \\Sigma V^T \\iff || A - U \\Sigma V^T ||_2 \\approx 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The numerical rank of $A$\n",
    "> **Exercise:** Compute the rank of $A$ by using the formula:\n",
    "> $ rk(A) = r \\text{ s.t. } \\sigma_r > 0, \\sigma_{r+1} = 0$,\n",
    "> and compare it with the output of the built-in function in `numpy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $k$-rank approximation\n",
    "> **Exercise:** Compute the $k$-rank approximation $A_k$ of $A$. Specifically, write a Python function that takes as input an integer $k \\leq \\min \\{ m, n \\}$ and computes the $k$-rank approximation $A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T$. Then, test it on a matrix of your preference and compute the approximation error in Frobenius norm, $\\| A - A_k \\|_F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD for Image Compression\n",
    "From a computational point of view, a grey-scale image is a **matrix** with shape `(height, width)`, such that the element in position $i, j$ contains the intensity of the pixel in the corresponding position. An RGB image is a triplet of matrices such that in position $i, j$, each of the three matrices represents the amount of Red, Green and Blue in the corresponding pixel.\n",
    "\n",
    "To work with images, we consider the `skimage.data` submodule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "# Loading the \"cameraman\" image\n",
    "x = skimage.data.camera()\n",
    "\n",
    "# Printing its shape\n",
    "print(f\"Shape of the image: {x.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize a matrix as an image, it can be used the `plt.imshow()` function from the `matplotlib.pyplot` module. If the image is a grey-scale image (as it is the case for the `camera` image we are considering), it is required to set the `cmap='gray'` option to visualize it as a grey-scale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the visualization, the image is still a matrix and all the techniques developed for matrices can be used on images. In particular, the $k$-rank approximation, by truncating the least important informations of the image, can be used to compress it with minimal information loss.\n",
    "\n",
    "We recall that the $k$-rank approximation $X_k$ of the image $X$ is defined as\n",
    "\n",
    "$$\n",
    "    X_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T\n",
    "$$\n",
    "\n",
    "where each $\\sigma_i$ is a scalar number, $u_i$ is an $m$-dimensional vector, while $v_i$ is an $n$-dimensional vector. As a consequence, the number of values required to memorize $X_k$ is $k(m + n + 1)$, while the number of values required to memorize the whole image $X$ is $mn$. As a consequence, the compression factor (i.e. the percentage of pixels we saved in memorizing $X_k$ instead of $X$) can be computed as:\n",
    "\n",
    "$$\n",
    "    c_k = 1 - \\frac{k (m + n + 1)}{mn}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Using the function defined in the previous exercise, compute the $k$-rank approximation of the cameraman image $X$ for different values of $k$ and observe the behavior of each reconstruction. Also, compute and plot the compression factor $c_k$ for each value of $k$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
