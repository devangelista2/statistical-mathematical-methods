{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "![](/regression_classification/fig/ML_diagram.png)\n",
    "\n",
    "Regression is a group of supervised learning techniques, alternative to classification. In classification, given a datapoint $x \\in \\mathbb{R}^n$, the task was to learn a model $y = f_\\theta(x)$ such that $y$ represents one of the $K$ possible classes in which $x$ lies. On the other side, a Machine Learning problem is a **Regression** when the target variable $y \\in \\mathbb{R}$ is a **continuous** variable, and the task is to approximately *interpolate* between the $y$ values, with the intent of being able to predict new outcome of $y$ when a new $x$ is given as input.\n",
    "\n",
    "![](/regression_classification/fig/regression_vs_classification.png)\n",
    "\n",
    "A part from that, the computation procedure to define a regression model is similar to the procedure required for a classification model. Indeed, let $f_\\theta(x)$ be a parametric function of $x \\in \\mathbb{R}^n$, parameterized by $\\theta \\in \\mathbb{R}^d$. Given a dataset $(X, Y)$ with:\n",
    "\n",
    "$$\n",
    "    X = [x^1 x^2 \\dots x^N] \\in \\mathbb{R}^{n \\times N},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "    Y = [y^1 y^2 \\dots y^N] \\in \\mathbb{R}^N,\n",
    "$$\n",
    "\n",
    "the task is to **train** the model (i.e. to optimize the parameters $\\theta$), such that $f_\\theta(x^i) \\approx y^i$ for any $i = 1, \\dots, N$. Clearly, this is done by solving the optimization problem:\n",
    "\n",
    "$$\n",
    "    \\theta^* = \\arg\\min_{\\theta \\in \\mathbb{R}^d} \\ell(\\theta; X, Y).\n",
    "$$\n",
    "\n",
    "Here, $\\ell(\\theta; X, Y)$ is a **loss function**, which describes how to optimize the parameters to achieve the regression task. When $n>1$, $f_\\theta(x)$ is called **multivariate regression model**, while if $n=1$, $f_\\theta(x)$ is an **univariate regression model**. For simplicity, from now on we will work with univariate regression models, where $x^i \\in \\mathbb{R}$ for any $i = 1, \\dots, N$, and consequently $X = [x^1 x^2 \\dots x^N] \\in \\mathbb{R}^N$.\n",
    "\n",
    "```{note}\n",
    "Note that we slightly changed the notation compared to the previous chapters. Indeed, the dimensionality of each input data vector $x$ is now named $n$ instead of $d$, which now refers to the dimension of the parameter vector. This choice will be clearer later, and it has been done to make the code more coherent through the lecture topics, since now our focus is on the variable $\\theta$, and not on $x$, which will be assumed to have dimension 1.\n",
    "```\n",
    "\n",
    "## Linear Regression\n",
    "The simplest regression model we can think of is the Linear Regression Model, defined as\n",
    "\n",
    "$$\n",
    "    f_\\theta(x) = \\theta_1 + \\theta_2 x = \\theta^T \\hat{x}\n",
    "$$\n",
    "\n",
    "where $\\theta = [\\theta_1, \\theta_2]^T$ and $\\hat{x} = [1, x]^T$. It defines a straight line, which approximates the data after the parameters have been optimized. Clearly, the resulting model will be correct if and only if the real function $y = f(x)$ generating the data is linear. Unfortunately, even in situations where $f(x)$ can be considered linear, the collected data $y$ is always corrupted by noise, which consequently breaks the linearity.\n",
    "\n",
    "Let's see for example what happens when we consider data such that $y = 2x$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Define arbitrarly x\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define arbitrarly x\n",
    "x = np.linspace(0, 1, 10)\n",
    "y = 2 * x\n",
    "\n",
    "# Visualize the data\n",
    "plt.plot(x, y, 'o')\n",
    "plt.title(\"Data given by y = 2x.\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly possible to perfectly approximate these data by a linear model. Indeed, if $\\theta = [0, 2]$, then:\n",
    "\n",
    "$$\n",
    "f_\\theta(x) = \\theta_1 + \\theta_2 x = 0 + 2x = 2x,\n",
    "$$\n",
    "\n",
    "which will match exactly the function that generates the data. \n",
    "\n",
    "Unfortunately, the same won't hold if the collected data is corrupted by noise (as it is common) in practical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Consider a little bit of noise\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m y_noisy = \u001b[43my\u001b[49m + np.random.normal(loc=\u001b[32m0\u001b[39m, scale=\u001b[32m0.2\u001b[39m, size=y.shape)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Visualize the noisy data\u001b[39;00m\n\u001b[32m      5\u001b[39m plt.plot(x, y_noisy, \u001b[33m'\u001b[39m\u001b[33mo\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "# Consider a little bit of noise\n",
    "y_noisy = y + np.random.normal(loc=0, scale=0.2, size=y.shape)\n",
    "\n",
    "# Visualize the noisy data\n",
    "plt.plot(x, y_noisy, 'o')\n",
    "plt.title(\"Noisy data given by y = 2x.\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want our model to approximately learn the data, somehow ignoring the noise. Luckily, at least when the noise is Gaussian distributed, this is usually what happens. \n",
    "\n",
    "## Training a model\n",
    "As we will see in greater detail in the next section, **training** a linear regression model means solving the optimization problem:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\min_{\\theta} \\ell(\\theta; X, Y).\n",
    "$$\n",
    "\n",
    "Where the loss function $\\ell(\\theta; X, Y)$ is usually chosen to be the **Mean Squared Error (MSE)** which, for a linear regression model, is defined as:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta; X, Y) = \\frac{1}{N} \\sum_{i=1}^N (f_\\theta(x^i) - y^i)^2 = \\frac{1}{N} \\sum_{i=1}^N (\\theta_1 + \\theta_2 x^i - y^i)^2.\n",
    "$$\n",
    "\n",
    "Note that this is a simple least squares problem, which can be written in matrix form as:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\min_{\\theta} \\underbrace{\\frac{1}{N} || X^T \\theta - Y ||_2^2}_{\\ell(\\theta; X, Y)},\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix} 1 & 1 & \\dots & 1 \\\\ x^1 & x^2 & \\dots & x^N \\end{bmatrix} \\in \\mathbb{R}^{2 \\times N}, \\qquad Y = \\begin{bmatrix} y^1 & y^2 & \\dots & y^N \\end{bmatrix} \\in \\mathbb{R}^N.\n",
    "$$\n",
    "\n",
    "Therefore, it can be solved either by normal equation method, solving:\n",
    "\n",
    "$$\n",
    "XX^T \\theta = XY,\n",
    "$$\n",
    "\n",
    "or by Gradient Descent, by observing that:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\ell(\\theta; X, Y) = X(X^T \\theta - Y).\n",
    "$$\n",
    "\n",
    "> **Exercise:** Write down an algorithm that solves the linear regression task on the noisy data given above. Use Normal Equation method to train it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "Linear regression model is too rigid to correctly approximate the data when the relationship between the input and output variables $x$ and $y$ is complex. \n",
    "\n",
    "To see this, take a look at the data contained into the `poly_regression_small.csv` file, which can be downloaded from Virtuale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"../code/data/poly_regression_small.csv\")\n",
    "x, y = data[\"x\"], data[\"y\"]\n",
    "\n",
    "# Visualize the data\n",
    "plt.plot(x, y, 'o')\n",
    "plt.title(\"Complex data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, no straight line will be able to correctly approximate it. \n",
    "\n",
    "> **Exercise:** Train a linear regression model to approximate the data above, and visualize the prediction.\n",
    "\n",
    "To make the model more flexible and able to capture the complexity of the distribuition of this data, we can consider a non-linear regression model. In particular, given a number $K > 0$, define:\n",
    "\n",
    "$$\n",
    "    f_\\theta(x) = \\sum_{j=0}^K \\phi_j(x) \\theta_j,\n",
    "$$\n",
    "\n",
    "where the functions $\\phi_1, \\phi_2, \\dots, \\phi_K$ are called **feature vectors**. Note that, for $K = 2$ and $\\phi_0(x) = 1$, $\\phi_1(x) = x$, we recover the linear regression model $f_\\theta(x) = \\theta_1 + \\theta_2 x$. For different values of $K$ and different feature vectors $\\phi_j(x)$, we get different regression model.\n",
    "\n",
    "Note that, if $\\theta = [\\theta_1, \\theta_2, \\dots, \\theta_K]^T$ and $\\phi(x) = [\\phi_1(x), \\phi_2(x), \\dots, \\phi_K(x)]^T$, then\n",
    "\n",
    "$$\n",
    "    f_\\theta(x) = \\phi^T(x) \\theta\n",
    "$$\n",
    "\n",
    "which implies that $f_\\theta(x)$ is a *linear function* in $\\theta$, for any choice of $\\phi_j(x)$. The *non-linearity* is therefore only as a function of $x$.\n",
    "\n",
    "A classical choice for the feature vector is $\\phi_j(x) = x^{j-1}$. In this way, for a given $K>0$, $\\phi(x)$ represents the vector of the first $K$ monomial in $x$, i.e.\n",
    "\n",
    "$$\n",
    "    \\phi(x) = [1, x, x^2, \\dots, x^{K-1}]^T\n",
    "$$\n",
    "\n",
    "and $f_\\theta(x)$ is a $K-1$-th degree polynomial. For this reason, $f_\\theta(x)$ is called **polynomial regression model** for this choice of $\\phi_j(x)$. \n",
    "\n",
    "When the loss function $\\ell(\\theta; X, Y)$ is the Mean Squared Error, training a polynomial regression model is similar to the training of the linear regression model. In particular, if:\n",
    "\n",
    "$$\n",
    "\\Phi(X) = \\begin{bmatrix} 1 & 1 & \\dots & 1 \\\\ x^1 & x^2 & \\dots & x^N \\\\ (x^1)^2 & (x^2)^2 & \\dots & (x^N)^2 \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ (x^1)^{K-1} & (x^2)^{K-1} & \\dots & (x^N)^{K-1} \\end{bmatrix}, \\qquad Y = \\begin{bmatrix} y^1 & y^2 & \\dots & y^N \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta; X, Y) = \\frac{1}{N} || \\Phi(X)^T \\theta - Y ||_2^2,\n",
    "$$\n",
    "\n",
    "whose normal equations read:\n",
    "\n",
    "$$\n",
    "\\Phi(X)\\Phi(X)^T \\theta = \\Phi(X) Y.\n",
    "$$\n",
    "\n",
    "> **Exercise:** Train a polynomial regression model over the data above. Try different values for the polynomial degree $K$. Compare the results with the polynomial: $f(x) = 4x^2 - 3x^4$. What do you observe for large values of $K$ (e.g. $K=8$)?\n",
    "> Repeat the experiment by training it on the data from `poly_regression_large.csv`. Do you observe something?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}