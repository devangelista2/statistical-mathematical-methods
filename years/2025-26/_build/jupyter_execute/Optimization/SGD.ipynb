{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "\n",
    "When training a Machine Learning model, we typically deal with a dataset  \n",
    "\n",
    "$$\n",
    "(X, Y) = \\{ (x^{(i)}, y^{(i)}) \\}_{i=1}^N,\n",
    "$$\n",
    "\n",
    "and a **parametric model** $ f_\\Theta(x) $ whose parameters $\\Theta \\in \\mathbb{R}^d$ (also called **weights**) must be learned.  \n",
    "The **training phase** is formulated as an **optimization problem**, where we seek parameters $\\Theta$ such that\n",
    "\n",
    "$$\n",
    "f_\\Theta(x^{(i)}) \\approx y^{(i)} \\quad \\text{for all } i=1,\\dots,N.\n",
    "$$\n",
    "\n",
    "To quantify this approximation, we introduce a **loss function** $\\mathcal{L}(\\Theta) := \\mathcal{L}(\\Theta; X, Y)$, which measures how far the model predictions are from the true targets.\n",
    "\n",
    "## Empirical Risk Minimization\n",
    "\n",
    "In most ML problems, the loss can be expressed as a sum (or average) over **sample-wise losses**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta; X, Y) = \\frac{1}{N} \\sum_{i=1}^N \\ell_i(\\Theta; x^{(i)}, y^{(i)}),\n",
    "$$\n",
    "\n",
    "where $\\ell_i$ quantifies the error for the $i$-th training sample.\n",
    "\n",
    "The training optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\Theta^* = \\arg\\min_\\Theta \\, \\mathcal{L}(\\Theta; X, Y)\n",
    "         = \\arg\\min_\\Theta \\, \\frac{1}{N}\\sum_{i=1}^N \\ell_i(\\Theta; x^{(i)}, y^{(i)}).\n",
    "$$\n",
    "\n",
    "\n",
    "The **Gradient Descent (GD)** method (from previous section) updates the parameters iteratively as:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\Theta^{(0)} \\in \\mathbb{R}^d, \\\\\n",
    "\\Theta^{(k+1)} = \\Theta^{(k)} - \\eta_k \\, \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}; X, Y)\n",
    "= \\Theta^{(k)} - \\eta_k \\, \\frac{1}{N}\\sum_{i=1}^N \\nabla_\\Theta \\ell_i(\\Theta^{(k)}; x^{(i)}, y^{(i)}).\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Here $\\eta_k > 0$ denotes the **learning rate** (or step size).\n",
    "\n",
    "## Example: Mean Squared Error (MSE)\n",
    "\n",
    "A common loss in regression problems is the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta; X, Y) = \\frac{1}{N}\\sum_{i=1}^N (f_\\Theta(x^{(i)}) - y^{(i)})^2\n",
    "= \\sum_{i=1}^N \\underbrace{\\frac{1}{N}(f_\\Theta(x^{(i)}) - y^{(i)})^2}_{=: \\ell_i(\\Theta; x^{(i)}, y^{(i)})}.\n",
    "$$\n",
    "\n",
    "\n",
    "By the chain rule:\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\ell_i(\\Theta; x^{(i)}, y^{(i)}) \n",
    "= \\frac{2}{N}\\,(f_\\Theta(x^{(i)}) - y^{(i)}) \\, \\nabla_\\Theta f_\\Theta(x^{(i)}),\n",
    "$$\n",
    "\n",
    "so that:\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\mathcal{L}(\\Theta; X, Y) \n",
    "= \\frac{2}{N}\\sum_{i=1}^N (f_\\Theta(x^{(i)}) - y^{(i)})\\,\\nabla_\\Theta f_\\Theta(x^{(i)}).\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore, one iteration of Gradient Descent becomes:\n",
    "\n",
    "$$\n",
    "\\Theta^{(k+1)} = \\Theta^{(k)} - \\eta_k \\frac{2}{N}\\sum_{i=1}^N (f_\\Theta(x^{(i)}) - y^{(i)})\\,\\nabla_\\Theta f_\\Theta(x^{(i)}).\n",
    "$$\n",
    "\n",
    "\n",
    "## Motivation for Stochastic Gradient Descent\n",
    "\n",
    "While the computation of each $\\nabla_\\Theta \\ell_i(\\Theta; x^{(i)}, y^{(i)})$ is inexpensive, the full gradient $\\nabla_\\Theta \\mathcal{L}(\\Theta; X, Y)$ requires summing over all $N$ samples, which is a prohibitive cost when $N$ is very large.\n",
    "\n",
    "The **Stochastic Gradient Descent (SGD)** algorithm addresses this by replacing the **exact gradient** with a **stochastic approximation** computed on a random subset of data (a *mini-batch*).\n",
    "\n",
    "## The SGD Algorithm\n",
    "\n",
    "Let $N_\\text{batch} \\ll N$ be the **batch size**, and let $\\mathcal{M}_k \\subset \\{1,\\dots,N\\}$ denote a random subset of indices such that $|\\mathcal{M}_k| = N_\\text{batch}$.\n",
    "\n",
    "At each iteration $k$:\n",
    "\n",
    "1. **Sample a mini-batch:**  \n",
    "   Randomly select a subset $\\mathcal{M}_k$ (a **batch**) from the dataset (typically without replacement).\n",
    "\n",
    "2. **Compute the approximate gradient:**  \n",
    "   \n",
    "   $$\n",
    "   \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}; \\mathcal{M}_k)\n",
    "   = \\frac{1}{N_\\text{batch}}\\sum_{i\\in\\mathcal{M}_k} \\nabla_\\Theta \\ell_i(\\Theta^{(k)}; x^{(i)}, y^{(i)}).\n",
    "   $$\n",
    "\n",
    "\n",
    "3. **Update the parameters:**\n",
    "   \n",
    "   $$\n",
    "   \\Theta^{(k+1)} = \\Theta^{(k)} - \\eta_k \\, \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}; \\mathcal{M}_k).\n",
    "   $$\n",
    "\n",
    "\n",
    "4. **Repeat** until all data have been used once.  \n",
    "   When the entire dataset has been processed, we say one **epoch** of SGD has been completed.  \n",
    "   The algorithm is typically run for a fixed number $E$ of epochs.\n",
    "\n",
    "![](./fig/SGD.png)\n",
    "\n",
    "## Comparison with Full Gradient Descent\n",
    "\n",
    "| Property | Gradient Descent (GD) | Stochastic Gradient Descent (SGD) |\n",
    "|-----------|------------------------|------------------------------------|\n",
    "| Gradient computation | Full dataset | Random subset (mini-batch) |\n",
    "| Iteration cost | High (depends on $N$) | Low (depends on $N_\\text{batch}$) |\n",
    "| Variance | Deterministic | Stochastic |\n",
    "| Convergence | Smooth and stable | Noisy, but often faster |\n",
    "| Scalability | Poor for large datasets | Excellent |\n",
    "\n",
    "The stochasticity introduces **noise** in the parameter updates, but this noise can help the optimizer **escape shallow local minima** and often improves generalization.\n",
    "\n",
    "## Typical Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def SGD(f, grad_f, X, Y, Theta0, lr=1e-2, batch_size=32, epochs=10):\n",
    "    \"\"\"\n",
    "    Simplified Stochastic Gradient Descent (SGD) implementation.\n",
    "    f: loss function, grad_f: gradient wrt Theta\n",
    "    X, Y: dataset\n",
    "    Theta0: initial parameters\n",
    "    lr: learning rate\n",
    "    \"\"\"\n",
    "    Theta = Theta0.copy()\n",
    "    N = len(X)\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data indices\n",
    "        idx = np.random.permutation(N)\n",
    "        for start in range(0, N, batch_size):\n",
    "            batch_idx = idx[start:start+batch_size]\n",
    "            grad = grad_f(Theta, X[batch_idx], Y[batch_idx])\n",
    "            Theta -= lr * grad\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Remarks\n",
    "\n",
    "- The **batch size** controls the trade-off between computational cost and gradient accuracy.  \n",
    "  Small batches introduce more stochastic noise; large batches approach full GD behavior.  \n",
    "\n",
    "- The **learning rate** $\\eta_k$ is usually constant or follows a decaying schedule, e.g.\n",
    "  \n",
    "  $$\n",
    "  \\eta_k = \\frac{\\eta_0}{1 + \\gamma_k}, \\quad \\gamma > 0.\n",
    "  $$\n",
    "\n",
    "\n",
    "- In practice, modern optimizers (e.g. Adam, RMSProp) extend SGD with adaptive step-sizes and momentum, which we will discuss later in the course.\n",
    "\n",
    "> **Exercise (Mini-Batch Approximation):**  \n",
    "> For a $d$-dimensional input vector $x^{(i)}$, consider the function  \n",
    "> \n",
    "> $$\n",
    "> \\mathcal{L}(\\Theta; X, Y) = \\frac{1}{N}\\sum_{i=1}^N (\\Theta^T x^{(i)} - y^{(i)})^2,\n",
    "> $$\n",
    ">\n",
    "> where $\\Theta \\in \\mathbb{R}^d$.\n",
    ">\n",
    "> 1. For $d = 1$, define a synthetic dataset $X, Y$ where $X$ is a set of $N = 100$ evenly-spaced elements in the range $[0, 1]$, while $Y = 2X + 1 + e$, $e \\sim \\mathcal{N}(0, 0.01)$.\n",
    "> 2. Implement Gradient Descent and Stochastic Gradient Descent for this loss.  \n",
    "> 3. Compare the number of iterations and the computational cost.  \n",
    "> 4. Plot the evolution of the loss across epochs for different batch sizes ($N_\\text{batch} = 1, 10, N$).  \n",
    "\n",
    "> **Exercise (Variance of the Stochastic Gradient):**  \n",
    "> For the same loss of the previous exercise, compute the gradient over multiple random batches $\\mathcal{M}_k$ of the same size, and compare:  \n",
    "> \n",
    "> $$\n",
    "> \\mathrm{Var}\\big(\\nabla_\\Theta \\mathcal{L}(\\Theta; \\mathcal{M}_k)\\big)\n",
    "> $$\n",
    ">\n",
    "> as a function of the batch size.  \n",
    "> Discuss how increasing $N_\\text{batch}$ affects the variance and the convergence stability.  \n",
    "\n",
    "> **Exercise (Learning Rate Scheduling):**  \n",
    "> Implement SGD on the same dataset with three learning-rate strategies:  \n",
    "> constant $\\eta_k=\\eta_0$, exponentially decaying $\\eta_k = \\eta_0 \\, e^{-\\gamma k}$, and inverse scaling $\\eta_k = \\frac{\\eta_0}{1+\\gamma k}$.  \n",
    "> Compare convergence rates and discuss which choice balances speed and stability better.\n",
    "\n",
    "\n",
    "```{warning}\n",
    "SGD has some drawbacks compared to GD. In particular, there is no way to check whether it reached the convergence (since we can't obviously compute the gradient of $\\mathcal{L}(\\theta; X, Y)$ to check its distance from zero, as it is required for the first Stopping Criteria) and we can't use the backtracking algorithm, for the same reason. As a consequence, the algorithm will stop ONLY after reaching the fixed number of epochs, and we must set a good value for the step size $\\alpha_k$ by hand. Those problems are partially solved by recent algorithms like SGD with Momentum, Adam, AdaGrad, ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## A Complete Example: Multi-Linear Regression with GD and SGD\n",
    "\n",
    "To conclude our discussion on Gradient Descent and its stochastic variant, we now consider a **complete example** where we will implement both algorithms *from scratch* (i.e., using only `numpy`) to solve a **multi-linear regression** problem on a real dataset.\n",
    "\n",
    "This exercise will help you understand how both **Gradient Descent (GD)** and **Stochastic Gradient Descent (SGD)** behave in a real training scenario and how different choices (e.g., batch size, learning rate) affect convergence.\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "We will use the **\"House Prices – Advanced Regression Techniques\"** dataset from Kaggle: [https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)\n",
    "\n",
    "The dataset contains various features describing houses (e.g., number of rooms, area, year built) and the **sale price** as the target variable.\n",
    "\n",
    "For simplicity, we will use only a few continuous variables, for example:\n",
    "- `LotArea` (the area of the lot in square feet),\n",
    "- `OverallQual` (an overall quality rating),\n",
    "- `YearBuilt` (the construction year).\n",
    "\n",
    "Our goal is to predict the house price $y$ using a linear model:\n",
    "\n",
    "$$\n",
    "f_\\Theta(x) = \\Theta^T x,\n",
    "$$\n",
    "\n",
    "where $x \\in \\mathbb{R}^d$ is the feature vector (including a bias term), and $\\Theta = [\\Theta_1, \\Theta_2, \\Theta_3] \\in \\mathbb{R}^d$ are the model parameters.\n",
    "\n",
    "### 1. Problem Definition\n",
    "\n",
    "We define the **Mean Squared Error (MSE)** loss function as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta; X, Y) = \\frac{1}{N}\\sum_{i=1}^N (f_\\Theta(x^{(i)}) - y^{(i)})^2\n",
    "= \\frac{1}{N}\\|X\\Theta - Y\\|_2^2,\n",
    "$$\n",
    "\n",
    "where $X$ is the data matrix of shape $(N, d)$ and $Y$ the vector of targets. The gradient of the loss with respect to $\\Theta$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\mathcal{L}(\\Theta; X, Y) = \\frac{2}{N} X^T (X\\Theta - Y).\n",
    "$$\n",
    "\n",
    "\n",
    "### 2. Data Preprocessing\n",
    "\n",
    "The first step is to:\n",
    "- Load the dataset (you can download the `.csv` file from Kaggle),\n",
    "- Select only the numerical columns we are interested in,\n",
    "- Normalize them to have mean 0 and variance 1,\n",
    "- Add a **bias column** of ones to $X$.\n",
    "\n",
    "Before delving into the implementation details of these steps, let us discuss the motivation for the last two steps.\n",
    "\n",
    "#### Data Normalization\n",
    "\n",
    "Before applying any gradient-based optimizer (such as Gradient Descent, Stochastic Gradient Descent, or any variant), it is essential to **normalize or standardize the data**, because:\n",
    "\n",
    "1. **Balanced feature scales:**  \n",
    "   Features measured in different units (e.g., square meters, years, ratings) produce gradients of very different magnitudes, making the loss landscape highly anisotropic and ill-conditioned (as we already saw in the previous section).\n",
    "\n",
    "2. **Faster and smoother convergence:**  \n",
    "   Centering and scaling features makes the contours of the loss more spherical, helping gradient-based methods move more directly toward the minimum.\n",
    "\n",
    "Hence, we always **standardize both the features and the target** in regression tasks:\n",
    "\n",
    "$$\n",
    "x_j \\leftarrow \\frac{x_j - \\mu_j}{\\sigma_j}, \n",
    "\\quad\n",
    "y \\leftarrow \\frac{y - \\mu_y}{\\sigma_y}.\n",
    "$$\n",
    "\n",
    "After training, predictions can be **denormalized** back to the original scale.\n",
    "\n",
    "#### The bias column\n",
    "\n",
    "To simplify the implementation of the SGD method and make the resulting algorithm independent on the dimensionality $d$ of the input space, we consider the **matrix formulation** of the optimization problem, where the model $f_\\Theta(x^{(i)})$ reads:\n",
    "\n",
    "$$\n",
    "f_\\Theta(x^{(i)}) = \\Theta^T x^{(i)}.\n",
    "$$\n",
    "\n",
    "Note that, while this formulation leads to more flexible implementation, the resulting model loses access to the **bias** term (sometimes called **quote**) compared to the scalar formulation:\n",
    "\n",
    "$$\n",
    "f_\\Theta(x^{(i)}) = \\sum_{j=1}^d W_j x_j^{(i)} + b_i.\n",
    "$$\n",
    "\n",
    "To recover that term, a typical solution is to add an artificial dummy column on $X$, composed of all $1$s, so that:\n",
    "\n",
    "$$\n",
    "\\tilde{X} = [1, X],\n",
    "$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\n",
    "f_\\Theta(x^{(i)}) = \\Theta^T \\tilde{x}^{(i)} = \\Theta^T [1, x^{(i)}] = \\Theta_1 + \\Theta_{2:{d+1}}^T x^{(i)},\n",
    "$$\n",
    "\n",
    "effectively recovering the bias term.\n",
    "\n",
    "We can now move to implementing the data preprocessing step for our example problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementing Gradient Descent\n",
    "\n",
    "We now implement Gradient Descent using the formula\n",
    "\n",
    "$$\n",
    "\\Theta^{(k+1)} = \\Theta^{(k)} - \\eta_k \\, \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}; X, Y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implementing Stochastic Gradient Descent\n",
    "\n",
    "For SGD, at each iteration we approximate the full gradient by sampling a random **mini-batch** $\\mathcal{M}_k$ of size $N_\\text{batch}$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\mathcal{L}(\\Theta; \\mathcal{M}_k) = \\frac{2}{N_\\text{batch}} X_{\\mathcal{M}_k}^T (X_{\\mathcal{M}_k}\\Theta - Y_{\\mathcal{M}_k}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training and Comparison\n",
    "\n",
    "We can now train both algorithms and compare their convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Discussion\n",
    "\n",
    "- **Gradient Descent (GD)** computes the exact gradient at each iteration, ensuring smooth convergence, but is computationally expensive for large datasets.  \n",
    "- **SGD**, by using mini-batches, introduces stochastic noise in the updates but drastically reduces computational cost per iteration.  \n",
    "  This noise can even **help** avoid getting stuck in local minima (for non-convex problems) and improves generalization.  \n",
    "- The **learning rate** and **batch size** jointly control the trade-off between convergence speed and stability.\n",
    "\n",
    "You should experiment with:\n",
    "- Different values of `lr` (e.g., `1e-2`, `5e-4`),\n",
    "- Different batch sizes (`N_batch = 1, 32, 128, N`),\n",
    "\n",
    "### 7. Questions for Reflection\n",
    "\n",
    "1. Compare the convergence curves of GD and SGD. Which is smoother? Which converges faster in terms of time per epoch?  \n",
    "2. How does batch size influence the final convergence point and oscillations?  \n",
    "3. If you replace the fixed learning rate with a decaying schedule (e.g. $\\eta_k = \\frac{\\eta_0}{1+k}$), how does the behavior change?  \n",
    "4. Compute the norm of the final gradient $\\|\\nabla_\\Theta \\mathcal{L}(\\Theta^{(E)}; X, Y)\\|$ for both algorithms and interpret its value.\n",
    "5. Plot the value of $\\|\\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}; X, Y)\\|$ along the iterations for the two methods and discuss the behavior.\n",
    "\n",
    "This exercise consolidates all the concepts discussed so far:\n",
    "- decomposition of the loss as a sum over samples,\n",
    "- explicit computation of gradients,\n",
    "- iterative updates via Gradient Descent and SGD,\n",
    "- and the trade-offs between accuracy, stability, and computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam: Adaptive Moment Estimation\n",
    "\n",
    "**Adam** is an optimization algorithm that improves on standard SGD by using both **momentum** (to accelerate convergence) and **adaptive learning rates** (to handle parameters with different scales).\n",
    "\n",
    "Given a stochastic gradient at iteration $k$,\n",
    "\n",
    "$$\n",
    "g^{(k)} \\;=\\; \\nabla_\\Theta \\mathcal{L}\\big(\\Theta^{(k)}; \\mathcal{M}_k\\big),\n",
    "$$\n",
    "\n",
    "Adam maintains two **exponential moving averages (EMAs)**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m^{(k)} &= \\beta_1\\, m^{(k-1)} + (1-\\beta_1)\\, g^{(k)} &&\\text{(first moment: mean of gradients)}\\\\[4pt]\n",
    "v^{(k)} &= \\beta_2\\, v^{(k-1)} + (1-\\beta_2)\\, (g^{(k)})^2 &&\\text{(second moment: variance of gradients)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "After correcting the initialization bias:\n",
    "\n",
    "$$\n",
    "\\hat m^{(k)} = \\frac{m^{(k)}}{1-\\beta_1^k}, \n",
    "\\qquad \n",
    "\\hat v^{(k)} = \\frac{v^{(k)}}{1-\\beta_2^k},\n",
    "$$\n",
    "\n",
    "the parameter update is:\n",
    "\n",
    "$$\n",
    "\\Theta^{(k+1)} = \\Theta^{(k)} - \\eta\\, \\frac{\\hat m^{(k)}}{\\sqrt{\\hat v^{(k)}} + \\varepsilon} g^{(k)}.\n",
    "$$\n",
    "\n",
    "Typical choices: $\\beta_1=0.9, \\; \\beta_2=0.999, \\; \\varepsilon = 10^{-8}$.\n",
    "\n",
    "### Implementing Adam on the House Prices dataset\n",
    "\n",
    "We reuse the Kaggle *House Prices* dataset and we solve the same optimization problem as before with Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer (with mini-batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: SGD vs. Adam (with standardized data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "After standardization:\n",
    "\n",
    "- Both **SGD** and **Adam** converge smoothly and to nearly identical minima.  \n",
    "- **Adam** usually shows faster initial loss reduction due to its adaptive scaling of gradients.  \n",
    "- Without normalization, the gradient magnitudes differ across parameters, causing Adam’s per-parameter learning rates to explode or vanish.  \n",
    "- In normalized space, the curvature of the loss is more uniform, allowing adaptive methods like Adam to perform as intended.\n",
    "\n",
    "Finally, when producing predictions in the original scale, remember to **denormalize** the output:\n",
    "\n",
    "$$\n",
    "\\hat{y}_\\text{real} = \\hat{y}_\\text{normalized} \\cdot \\sigma_Y + \\mu_Y.\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}