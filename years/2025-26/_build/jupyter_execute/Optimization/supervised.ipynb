{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8eee968",
   "metadata": {},
   "source": [
    "# Supervised Learning for Classification\n",
    "\n",
    "In the previous chapters, we introduced the general Machine Learning pipeline and developed a detailed understanding of optimization techniques such as Gradient Descent and Stochastic Gradient Descent, applied in particular to **regression** problems. In regression, the goal is to predict a *continuous* quantity, such as the price of a house, the temperature of a city, or the value of a sensor measurement.\n",
    "\n",
    "We now shift our focus to another fundamental branch of supervised learning: **classification**.\n",
    "\n",
    "![](fig/classification_2.png)\n",
    "\n",
    "## From Regression to Classification\n",
    "\n",
    "In a **classification** problem, the objective is *not* to predict a continuous value, but instead to assign each input $x \\in \\mathbb{R}^d$ to one of a set of **discrete classes**. Typical examples include:\n",
    "\n",
    "- Classifying emails as *spam* or *not spam*,\n",
    "- Determining whether a medical image shows *healthy* tissue or a *tumor*,\n",
    "- Recognizing handwritten digits (0–9),\n",
    "- Predicting whether a customer will *buy* a product or *not*.\n",
    "\n",
    "In the simplest case (that is, binary classification), we aim to predict whether a label $y$ belongs to one of two classes, which we will denote by\n",
    "\n",
    "$$\n",
    "y \\in \\{0, 1\\}.\n",
    "$$\n",
    "\n",
    "\n",
    "This setup strongly resembles linear regression: we receive an input $x$ and produce an output $y = f_\\Theta(x)$. However, while regression outputs a real number, classification requires a **probability** or a **discrete decision**.\n",
    "\n",
    "For this reason, we will begin with a natural extension of linear regression to binary classification. The key idea is simple:\n",
    "\n",
    "> We compute a linear prediction as in regression, then pass it through a **sigmoid function** to obtain a probability in $[0,1]$.\n",
    "\n",
    "Formally,\n",
    "\n",
    "$$\n",
    "f_\\Theta(x) := \\sigma \\left( \\sum_{i=1}^n \\Theta_i x_i \\right) = \\sigma(\\Theta^T x),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\sigma(t) = \\frac{1}{1 + e^{-t}}\n",
    "$$\n",
    "\n",
    "is the **sigmoid function**, which maps any real number to a value between 0 and 1. We interpret $f_\\Theta(x)$ as the probability that $x$ is associated with the class $y = 1$, i.e.:\n",
    "\n",
    "$$\n",
    "\\sigma(\\Theta^T x) = \\mathbb{P}(y=1 \\mid x).\n",
    "$$\n",
    "\n",
    "Thus, we have transformed a linear regression model into a **probabilistic classifier**.\n",
    "\n",
    "This model is known as **logistic regression**, and it represents the fundamental building block for modern binary classifiers, including neural networks and deep learning models.\n",
    "\n",
    "Before deriving its training procedure, we first explore why the sigmoid is a good choice and how classification differs from regression in terms of modeling and objective functions.\n",
    "\n",
    "![](fig/sigmoid.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae447a",
   "metadata": {},
   "source": [
    "## The Sigmoid Function and Probabilistic Interpretation\n",
    "\n",
    "To move from regression to classification, we need a mechanism that transforms a real-valued output into a probability. The **sigmoid function** plays this role perfectly as it maps any real number $t \\in \\mathbb{R}$ into the interval $(0,1)$, making it ideal for modeling probabilities.\n",
    "\n",
    "### Why Sigmoid?\n",
    "\n",
    "Consider a linear model (such as the one viewed in the previous chapter),\n",
    "\n",
    "$$\n",
    "z = \\Theta^T x.\n",
    "$$\n",
    "\n",
    "Since $z$ can be negative or positive, we cannot interpret it directly as a probability. The sigmoid, however, has the following desirable properties:\n",
    "\n",
    "- **Range:** $\\sigma(t) \\in (0,1)$, which makes it easy to interpret its output as a probability,\n",
    "- **Monotonicity:** larger linear scores correspond to higher probabilities,\n",
    "- **Smoothness:** continuously differentiable (needed for gradient methods),\n",
    "\n",
    "### Logistic Model Interpretation\n",
    "\n",
    "We interpret the model output as\n",
    "\n",
    "$$\n",
    "\\sigma(\\Theta^T x) = \\mathbb{P}(y=1 \\mid x;\\Theta).\n",
    "$$\n",
    "\n",
    "\n",
    "The **decision rule** for binary classification becomes:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\sigma(\\Theta^T x) \\ge 0.5, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "This creates a **decision boundary** given by\n",
    "\n",
    "$$\n",
    "\\Theta^T x = 0,\n",
    "$$\n",
    "\n",
    "which is a hyperplane, exactly as in linear regression, meaning logistic regression produces a **linear classifier**.\n",
    "\n",
    "### Geometric Interpretation\n",
    "\n",
    "The sign of the linear function determines the predicted class. The sigmoid simply “softens” this decision, providing a *probability* rather than a hard assignment.\n",
    "\n",
    "Intuitively:\n",
    "\n",
    "- If $\\Theta^T x \\gg 0$: high confidence in class 1  \n",
    "- If $\\Theta^T x \\ll 0$: high confidence in class 0  \n",
    "- If $\\Theta^T x \\approx 0$: uncertain prediction ($\\approx$0.5 probability)\n",
    "\n",
    "### Gradient of Sigmoid\n",
    "\n",
    "To train the model via gradient methods, we need its derivative:\n",
    "\n",
    "$$\n",
    "\\sigma'(t) = \\sigma(t)(1 - \\sigma(t)).\n",
    "$$\n",
    "\n",
    "This elegant form (a product of the output and its complement) explains why the sigmoid is extremely convenient for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcff27f",
   "metadata": {},
   "source": [
    "## Binary Cross-Entropy Loss\n",
    "\n",
    "Now that we model the probability of a sample belonging to class 1 as\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(\\Theta^T x),\n",
    "\\qquad \n",
    "\\hat{y} \\in (0,1),\n",
    "$$\n",
    "\n",
    "we need an appropriate loss function to measure how well the model matches the true labels.\n",
    "\n",
    "Since classification outputs *probabilities*, the correct loss is derived from **maximum likelihood** and corresponds to the **binary cross-entropy** (also called logistic loss). Let $y \\in \\{0,1\\}$ be the true label. We interpret the model as assigning the probability\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(y = 1 \\mid x;\\Theta) = \\hat{y}, \n",
    "\\qquad \n",
    "\\mathbb{P}(y = 0 \\mid x;\\Theta) = 1 - \\hat{y}.\n",
    "$$\n",
    "\n",
    "Thus, the likelihood of observing $y$ is\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(y \\mid x;\\Theta)\n",
    "= \\hat{y}^y \\, (1 - \\hat{y})^{(1-y)}.\n",
    "$$\n",
    "\n",
    "Maximizing the likelihood is equivalent to minimizing the **negative log-likelihood**:\n",
    "\n",
    "$$\n",
    "\\ell(\\Theta; x, y)\n",
    "= - \\left( y \\log \\hat{y} + (1 - y) \\log (1 - \\hat{y}) \\right).\n",
    "$$\n",
    "\n",
    "Substituting $\\hat{y} = f_\\Theta(x) = \\sigma(\\Theta^T x)$, the **binary cross-entropy (BCE)** loss becomes\n",
    "\n",
    "$$\n",
    "\\ell(\\Theta; x, y)\n",
    "= - \\left(\n",
    "y \\log \\sigma(\\Theta^T x)\n",
    "+ (1-y) \\log \\left(1 - \\sigma(\\Theta^T x)\\right)\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "For a dataset $(X,Y) = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$, the empirical loss is therefore:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta; X, Y)\n",
    "= \\frac{1}{N} \\sum_{i=1}^N \\ell(\\Theta; x^{(i)}, y^{(i)}).\n",
    "$$\n",
    "\n",
    "### Why Cross-Entropy?\n",
    "\n",
    "Cross-entropy is preferred over MSE for classification because:\n",
    "\n",
    "- It directly measures **log-likelihood** (statistically principled),\n",
    "- It penalizes confident wrong predictions more strongly,\n",
    "- Its gradients remain informative even when $\\sigma(\\Theta^T x)$ saturates,\n",
    "- Empirically, it leads to **much faster and more reliable training**.\n",
    "\n",
    "You can think of BCE as telling the model:\n",
    "\n",
    "> “Assign high probability to true labels and low probability to incorrect ones, and pay a high price if you are confidently wrong.”\n",
    "\n",
    "### Gradient Derivation\n",
    "\n",
    "We train the classification model with a variant of Stochastic Gradient Descent (SGD). To do that, we first need to derive the gradient needed for optimization. Recall that:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta; X, Y) = \\sum_{i=1}^N \\ell (\\Theta; x^{(i)}, y^{(i)}),\n",
    "$$\n",
    "\n",
    "and therefore:\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\mathcal{L}(\\Theta; X, Y) = \\sum_{i=1}^N \\nabla_\\Theta \\ell (\\Theta; x^{(i)}, y^{(i)}),\n",
    "$$\n",
    "\n",
    "for which we only need to compute $\\nabla_\\Theta \\ell (\\Theta; x^{(i)}, y^{(i)})$ to run the SGD algorithm. To do that, let:\n",
    "\n",
    "$$\n",
    "\\hat y = \\sigma(z),\\qquad z = \\Theta^T x,\\qquad \n",
    "\\ell(\\Theta; x,y) = -\\Big[y\\log \\hat y + (1-y)\\log(1-\\hat y)\\Big].\n",
    "$$\n",
    "\n",
    "We apply the chain rule:\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\ell \\;=\\; \\frac{\\partial \\ell}{\\partial z}\\,\\frac{\\partial z}{\\partial \\Theta}\n",
    "\\;=\\; \\frac{\\partial \\ell}{\\partial z}\\,x,\n",
    "$$\n",
    "\n",
    "since $\\partial z/\\partial \\Theta = x$. Differentiating $\\ell$ w.r.t. $z$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial z}\n",
    "= -\\left[\n",
    "y \\cdot \\frac{1}{\\hat y}\\cdot \\frac{\\partial \\hat y}{\\partial z}\n",
    "+ (1-y)\\cdot \\frac{-1}{1-\\hat y}\\cdot \\frac{\\partial \\hat y}{\\partial z}\n",
    "\\right]\n",
    "= -\\frac{\\partial \\hat y}{\\partial z}\n",
    "\\left[\n",
    "\\frac{y}{\\hat y} - \\frac{1-y}{1-\\hat y}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "\n",
    "Using that $\\sigma'(z)=\\hat y(1-\\hat y)$, we have $\\dfrac{\\partial \\hat y}{\\partial z}=\\hat y(1-\\hat y)$. Substitute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial z}\n",
    "= -\\hat y(1-\\hat y)\\left[\n",
    "\\frac{y}{\\hat y} - \\frac{1-y}{1-\\hat y}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "\n",
    "Simplify the bracket:\n",
    "\n",
    "$$\n",
    "\\frac{y}{\\hat y} - \\frac{1-y}{1-\\hat y}\n",
    "= \\frac{y(1-\\hat y) - (1-y)\\hat y}{\\hat y(1-\\hat y)}\n",
    "= \\frac{y - y\\hat y - \\hat y + y\\hat y}{\\hat y(1-\\hat y)}\n",
    "= \\frac{y - \\hat y}{\\hat y(1-\\hat y)}.\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial z}\n",
    "= -\\hat y(1-\\hat y)\\cdot \\frac{y - \\hat y}{\\hat y(1-\\hat y)}\n",
    "= -(y - \\hat y)\n",
    "= \\hat y - y.\n",
    "$$\n",
    "\n",
    "\n",
    "Plugging back into the chain rule:\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\ell(\\Theta; x, y)\n",
    "= \\frac{\\partial \\ell}{\\partial z}\\,x\n",
    "= (\\hat y - y)\\,x.\n",
    "$$\n",
    "\n",
    "Therefore, for any batch $\\mathcal{M} \\subseteq \\{ X, Y \\}$,\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\mathcal{L}(\\Theta; X, Y) = \\sum_{i=1}^N (\\sigma(\\Theta^T x^{(i)}) - y^{(i)})\\,x^{(i)}.\n",
    "$$\n",
    "\n",
    "This is **very convenient**:\n",
    "- It has the same form as linear regression,\n",
    "- Only the error term changes: $\\hat{y}^{(i)} - y^{(i)}$,\n",
    "- The sigmoid derivative magically cancels during simplification.\n",
    "\n",
    "Thus, the SGD / GD update rule becomes\n",
    "\n",
    "$$\n",
    "\\Theta^{(k+1)} = \\Theta^{(k)} - \\eta_k\\,(\\sigma(\\Theta^{(k), T} x^{(i)}) - y^{(i)})\\,x^{(i)}.\n",
    "$$\n",
    "\n",
    "\n",
    "This simplicity is one reason logistic regression remains foundational.\n",
    "\n",
    "### Interpretation of the Gradient\n",
    "\n",
    "- If the model predicts a probability **too high** for label 1, it pushes $\\Theta$ **down**.\n",
    "- If the probability is **too low**, it pushes $\\Theta$ **up**.\n",
    "- The update magnitude depends on the **confidence error** and the **input**.\n",
    "\n",
    "Logistic regression is thus **a smooth, probabilistic, differentiable extension of linear classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce1cc8",
   "metadata": {},
   "source": [
    "## Evaluating Classification Models\n",
    "\n",
    "In regression we measured performance using metrics such as Mean Squared Error. For classification, the target is no longer a real number, but a **category**, so evaluation metrics must reflect **decision quality**, not numerical closeness.\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "Given binary labels $y \\in \\{0,1\\}$ and predictions $\\hat{y} \\in \\{0,1\\}$, we define:\n",
    "\n",
    "| Actual \\ Predicted | 1 (Positive) | 0 (Negative) |\n",
    "|--------------------|--------------|--------------|\n",
    "| **1 (Positive)**   | True Positive (TP) | False Negative (FN) |\n",
    "| **0 (Negative)**   | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "These four numbers form the **confusion matrix** and are the basis for all metrics below.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "$$\n",
    "\\text{Acc}\n",
    "= \\frac{\\text{TP} + \\text{TN}}{\\text{TP + TN + FP + FN}}\n",
    "$$\n",
    "\n",
    "Accuracy answers: *\"What fraction of total predictions are correct?\"*\n",
    "\n",
    "**Limitation:** Alone, it can be misleading when classes are very unbalanced  \n",
    "(e.g., always predicting “0” in a dataset that is 99% zeros gives 99% accuracy, but is useless).  \n",
    "*(We ignore class imbalance handling for now, but this explains why accuracy is often not enough.)*\n",
    "\n",
    "### Precision and Recall\n",
    "\n",
    "Precision:\n",
    "\n",
    "$$\n",
    "\\text{Precision}\n",
    "= \\frac{\\text{TP}}{\\text{TP + FP}}\n",
    "$$\n",
    "\n",
    "*\"Of the samples predicted as positive, how many are actually positive?\"*\n",
    "\n",
    "Recall (a.k.a. True Positive Rate):\n",
    "\n",
    "$$\n",
    "\\text{Recall}\n",
    "= \\frac{\\text{TP}}{\\text{TP + FN}}\n",
    "$$\n",
    "\n",
    "*\"Of all positive samples, how many did we correctly detect?\"*\n",
    "\n",
    "Different applications emphasize different metrics:\n",
    "\n",
    "- Spam detection: **High precision** (avoid marking real emails as spam)\n",
    "- Disease screening: **High recall** (catch as many sick patients as possible)\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "The **harmonic mean** of precision and recall:\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}\n",
    "{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "\n",
    "Useful when you need a single number summarizing both precision and recall.\n",
    "\n",
    "### Implementation Exercise: Evaluate Logistic Regression\n",
    "\n",
    "After training your logistic regression model via SGD:\n",
    "\n",
    "1. Compute $\\hat{y} = \\sigma(\\Theta^T x)$\n",
    "2. Convert to predictions $\\hat{y} \\in \\{0,1\\}$ using a threshold (start with 0.5)\n",
    "3. Compute:\n",
    "   - Confusion matrix\n",
    "   - Accuracy\n",
    "   - Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa76b6",
   "metadata": {},
   "source": [
    "## Example: Logistic Regression on a Real Dataset\n",
    "\n",
    "To conclude this chapter, we train a logistic regression model **from scratch (`numpy` only)** on a real classification dataset from Kaggle:\n",
    "\n",
    "**Pima Indians Diabetes Database**: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database\n",
    "\n",
    "This dataset contains medical measurements for female patients and a binary label indicating whether diabetes was diagnosed. Similarly to what we previously did for regression, we will:\n",
    "\n",
    "- Load the dataset\n",
    "- Standardize features\n",
    "- Train logistic regression with SGD\n",
    "- Evaluate accuracy\n",
    "\n",
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d12bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Download `diabetes.csv` from Kaggle and place it in notebook directory\n",
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "\n",
    "X = df.drop(\"Outcome\", axis=1).values   # features\n",
    "y = df[\"Outcome\"].values.reshape(-1,1)  # labels\n",
    "\n",
    "# Standardize features\n",
    "X_mean, X_std = X.mean(axis=0), X.std(axis=0)\n",
    "X = (X - X_mean) / X_std\n",
    "\n",
    "# Add bias term\n",
    "X = np.hstack([np.ones((X.shape[0],1)), X])\n",
    "N, d = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ada86",
   "metadata": {},
   "source": [
    "### Logistic Regression (SGD Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d7e54",
   "metadata": {},
   "source": [
    "### Adam Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663056e1",
   "metadata": {},
   "source": [
    "## From Logistic Regression to a Simple Neural Network\n",
    "\n",
    "![](fig/NN.png)\n",
    "\n",
    "Up to this point, our models have been **linear**: for both regression and classification we always computed a single linear score of the form\n",
    "\n",
    "$$\n",
    "z = \\Theta^T x,\n",
    "\\qquad\n",
    "\\hat y = \\sigma(z),\n",
    "$$\n",
    "\n",
    "and then trained the parameters by minimizing an appropriate loss. This is the core of **logistic regression**, and as we have seen, it already performs surprisingly well. However, a single linear transformation can only separate data that is linearly separable.  \n",
    "In other words, logistic regression can only learn **linear decision boundaries**. But what if the true relationship between the input and the output is more complex? For example, what if the classes lie on *curved* or *intertwined* regions of space? This motivates the next step: **neural networks**.\n",
    "\n",
    "### Adding a Hidden Layer\n",
    "\n",
    "The idea behind neural networks is very simple:\n",
    "\n",
    "> Instead of directly mapping input features to an output, we first transform the inputs into a richer representation, and then we make predictions based on that representation.\n",
    "\n",
    "We do this by inserting a **hidden layer**. Concretely, we take our input $x$, apply a **linear map**, then a **nonlinear activation function**, and use the resulting features to make the final prediction. Mathematically, instead of\n",
    "\n",
    "$$\n",
    "\\hat y = \\sigma(\\Theta^T x),\n",
    "$$\n",
    "\n",
    "we compute\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_1 &= W_1 x + b_1,\\\\\n",
    "a_1 &= \\mathrm{ReLU}(z_1),\\\\\n",
    "z_2 &= W_2 a_1 + b_2,\\\\\n",
    "\\hat y &= \\sigma(z_2),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $W_1, b_1$ are the parameters of the **hidden layer**,\n",
    "- $W_2, b_2$ are the parameters of the **output (logistic) layer**,\n",
    "- $\\mathrm{ReLU}(t) = \\max(0, t)$ introduces **nonlinearity**.\n",
    "\n",
    "This model can now learn **non-linear decision boundaries**, because the ReLU transformation can bend and reshape the input space before classification happens.\n",
    "\n",
    "Notice how intuitive this is: the ReLU layer is learning new features automatically, instead of us hand-engineering them.\n",
    "\n",
    "### Training the Network: The Chain Rule in Action\n",
    "\n",
    "Even though the model is more complex, the training principle remains unchanged:\n",
    "\n",
    "> we compute the loss, and then adjust the parameters to reduce it.\n",
    "\n",
    "The key difference is that we now have more parameters and a composition of functions. Luckily, differentiation rules scale perfectly: we simply apply the **chain rule** layer by layer. This process, known as **backpropagation**, is nothing more than repeatedly applying:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} f(g(x)) = f'(g(x)) \\cdot g'(x).\n",
    "$$\n",
    "\n",
    "The ReLU nonlinearity also has a very simple derivative:\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}'(z) =\n",
    "\\begin{cases}\n",
    "1, & z > 0,\\\\\n",
    "0, & z \\le 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "During backpropagation, this means that gradients are allowed to “flow” only through neurons that were active (i.e., had positive input). Neurons that had negative inputs simply do not contribute.\n",
    "\n",
    "This simple idea already allows us to build a universal function approximator: a network with only one hidden layer can theoretically approximate **any continuous function**, provided we use enough hidden units.\n",
    "\n",
    "Of course, in practice, deeper networks tend to learn more effectively but that is the story for coming courses.\n",
    "\n",
    "### Implementation From Scratch\n",
    "\n",
    "Below is a compact, fully-vectorized implementation of a one-hidden-layer neural network for binary classification.\n",
    "\n",
    "This code:\n",
    "\n",
    "- Computes the forward pass,\n",
    "- Computes all gradients manually using the chain rule,\n",
    "- Applies SGD updates,\n",
    "- Logs training loss and accuracy.\n",
    "\n",
    "You will recognize the familiar structure from logistic regression, just with one more layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99fb40",
   "metadata": {},
   "source": [
    "### Final Remarks\n",
    "\n",
    "What we have built here is the simplest possible neural network. Yet, conceptually, it already contains **all the essential ingredients of deep learning**:\n",
    "\n",
    "- Stacking linear transformations,\n",
    "- Inserting nonlinearities,\n",
    "- Applying the chain rule backwards through the model,\n",
    "- Updating parameters via gradient-based optimization.\n",
    "\n",
    "Everything you may have heard about, such as convolutional networks, transformers, diffusion models, LLMs, is built on top of this same foundation.\n",
    "\n",
    "By making the jump from logistic regression to a one-hidden-layer network, we now start seeing how machines can **learn powerful, nonlinear patterns** from data, instead of relying on manually engineered features. From here, we will continue increasing expressivity, depth, and scalability, always guided by the same core mathematical principles introduced in this course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}