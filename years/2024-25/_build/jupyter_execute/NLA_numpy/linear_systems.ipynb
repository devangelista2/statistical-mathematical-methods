{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Linear Systems with Python\n",
    "In the following we want to learn how to use `numpy` and `scipy` to solve Linear Systems with Python. The majority of the functions in `numpy` and `scipy` for Numerical Linear Algebra are contained in the sub-packages `np.linalg` and `scipy.linalg`.\n",
    "\n",
    "As a general rule, remember that `np.linalg` contains all the basic functions to run NLA algorithms, while `scipy` mostly focus on efficiency, implementing algorithms such as fast linear system solvers, memory efficiet matrix memorization, ... \n",
    "\n",
    "To fix the notation, consider a matrix $A \\in \\mathbb{R}^{n \\times n}$ and a vector $y \\in \\mathbb{R}^n$. **Solving** a linear system means finding (when exists) a vector $x \\in \\mathbb{R}^n$ such that it satisfies\n",
    "\n",
    "$$\n",
    "    Ax = y.\n",
    "$$\n",
    "\n",
    "Clearly, a trivial solution to this problem is to compute the inverse of $A$ by using the function `np.linalg.inv()`, and then multiplying it by $y$ to obtain the solution:\n",
    "\n",
    "$$\n",
    "    x = A^{-1} y.\n",
    "$$\n",
    "\n",
    "However, we already remarked that this is unfeasible for large matrices $A$, since the computation of the inverse of a matrix requires $O(n!)$ operations, which grows insanely fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Implement a Python function taking as input a non-singular matrix $A \\in \\mathbb{R}^{n \\times n}$, a datum $y \\in \\mathbb{R}^n$, and returning the solution to the linear system $Ax = y$, by multiplying $A^{-1}$ to $y$. This approach is usually referred to as **direct solution**. Check the time required to run this algorithm by defining the matrix $A$ as a matrix of all ones with dimension $n \\times n$, $y$ being the vector of all ones, at increasing values of $n$ from 1 to 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the exercise above, computing the solution to the linear system requires a lot of time even for relatively small matrices. The computational time required to solve the system can be lowered from $O(n!)$ to $O(n^3)$ by using a much efficient algorithm.\n",
    "\n",
    "This is easy to do in `numpy`, since it implements a function `np.linalg.solve`, taking as input a 2-dimensional array `A` and a 1-dimensional array `y`, and returning the solution `x` to the linear system. In particular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Generates the problem\u001b[39;00m\n\u001b[32m      4\u001b[39m A = np.array([[\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m], [\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m], [\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m]])\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generates the problem\n",
    "A = np.array([[1, 1, 1], [2, 1, 2], [0, 0, 1]])\n",
    "y = np.array([0, 1, 0])\n",
    "\n",
    "# Solve the system\n",
    "x_sol = np.linalg.solve(A, y)\n",
    "print(f\"The solution is {x_sol}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the accuracy\n",
    "You should already know that a big limitation of any computational device is the **floating point system**. Indeed, the way of representing numbers by their mantissa+exponent decomposition, allows for a very efficient representation, but clearly not every floating point number is representable. For example, you already studied the concept of machine precision $\\epsilon$, i.e. the smallest number such that\n",
    "\n",
    "$$\n",
    "fl(1 + \\epsilon) > fl(1).\n",
    "$$\n",
    "\n",
    "As a consequence, a number such as $1 + \\frac{\\epsilon}{2}$ cannot be represented by computers, since it is indistinguishable from 1.\n",
    "\n",
    "This problem becomes particularly intense when one wants to encode real numbers such as $\\pi$ or the Nepero number $e$. Therefore, when we work with any algorithm which is implemented on a computational device, we always need to assume that the representation has some error. As a notation, we denote as $\\tilde{A}$ the machine representation of a matrix $A$, and by $\\delta A := \\tilde{A} - A$ the representation error. The same notation applies for vectors $x$ and $y$ representing the right-hand side of a linear system and its solution, respectively. \n",
    "\n",
    "An important aspect of developing NLA algorithm is to study how representation errors in the measurement matrix $A$ and known-term $y$ gets **amplified** (or **shrinked**) in the solution $x$ (i.e. to compute $\\delta x$ as a function of $\\delta A$ and $\\delta y$). In particular, we usually care about the **relative** error, which is simpler to compute in most of the cases, and it does not depend on the measuring unit of the quantities.\n",
    "\n",
    "In particular, the relative error between the true solution $x_{true}$ and the computed solution $x_{sol}$ is defined as:\n",
    "\n",
    "$$\n",
    "    E(x_{true}, x_{sol}) := \\frac{|| x_{true} - x_{sol} ||_2}{|| x_{true} ||_2} = \\frac{|| \\delta x ||_2}{|| x_{true} ||_2}.\n",
    "$$\n",
    "\n",
    "However, to compute the relative error of a computed solution $x_{sol}$, we first need to know the **true** solution $x_{true}$, which is usually not the case in real scenario. How can be achieve it? \n",
    "\n",
    "A common solution, which we will use for the whole course, is to verify the performance of the algorithm by building a **test problem**, i.e. a problem of which we know the true solution *by construction*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Test Problem\n",
    "Consider a matrix $A \\in \\mathbb{R}^{n \\times n}$ and assume we want to test the accuracy of an algorithm solving systems involving $A$. Fix an $n$-dimensional vector $x_{true} \\in \\mathbb{R}^n$, and compute $y = Ax_{true}$. Clearly, this procedure defines a linear system\n",
    "\n",
    "$$\n",
    "    Ax = y\n",
    "$$\n",
    "\n",
    "of which we know that $x_{true}$ is a **solution by construction**, since we built the term $y$ accordingly. Now, when we apply our algorithm to that linear system, we get a solution $x_{sol}$, which is in general different from $x_{true}$ due to the amplification of the errors coming from the numerical representation of $A$. Since we defined the system such that $x_{true}$ is the true solution, we can compute the relative error $E(x_{true}, x_{sol})$ asssociated to the solution obtained by the algorithm, and quantify how the algorithm performed in terms of error amplification. \n",
    "\n",
    "Let's see how we can build a test problem in Python to test the accuracy of the classical linear system solver algorithm `np.linalg.solve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Setting up the dimension\u001b[39;00m\n\u001b[32m      4\u001b[39m n = \u001b[32m15\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Setting up the dimension\n",
    "n = 15\n",
    "\n",
    "# Creating the test problem (with a Random matrix)\n",
    "A = np.random.randn(n, n) # n x n random matrix\n",
    "x_true = np.ones((n, ))   # n-dimensional vector of ones\n",
    "\n",
    "y = A @ x_true # Compute the term y s.t. x_true is a sol.\n",
    "\n",
    "# Solving the system with numpy\n",
    "x_sol = np.linalg.solve(A, y)\n",
    "\n",
    "# Computing the accuracy\n",
    "E_rel = np.linalg.norm(x_true - x_sol, 2) / np.linalg.norm(x_true, 2)\n",
    "print(f\"The relative error is {E_rel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the error is very small. This is due to two aspects of the problem:\n",
    "\n",
    "1. The algorithm is **stable** (i.e. it does not amplify the errors on $A$)\n",
    "2. The matrix $A$ il \"well-behaved\", i.e. its inverse does not amplifies the errors on $y$. We say that $A$ is **well-conditioned**, as opposed to **ill-conditioned** matrices.\n",
    "\n",
    "Let's check what happens when we apply the same, stable algorithm, to solve a linear system where the matrix $A$ is ill-conditioned, such as the `hilbert` matrix (available from the `scipy` package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "\n",
    "# Setting up the dimension\n",
    "n = 15\n",
    "\n",
    "# Creating the test problem (with Hilbert matrix)\n",
    "A = scipy.linalg.hilbert(n) # n x n hilbert matrix\n",
    "x_true = np.ones((n, ))   # n-dimensional vector of ones\n",
    "\n",
    "y = A @ x_true # Compute the term y s.t. x_true is a sol.\n",
    "\n",
    "# Solving the system with numpy\n",
    "x_sol = np.linalg.solve(A, y)\n",
    "\n",
    "# Computing the accuracy\n",
    "E_rel = np.linalg.norm(x_true - x_sol, 2) / np.linalg.norm(x_true, 2)\n",
    "print(f\"True solution: {x_true}.\")\n",
    "print(f\"Computed solution: {x_sol}.\")\n",
    "print(f\"The relative error is {E_rel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows a fundamental concept in Numerical Linear Algebra: using a stable algorithm does **NOT guarantee** that that solution to the problem is accurate. Indeed, if the system matrix $A$ is ill-conditioned, then there is **no algorithm** which is able to obtain an accurate solution.\n",
    "\n",
    "In the following section, we investigate more in detail the meaning of conditioning of a linear operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition number\n",
    "When the matrix $A$ is ill-conditioned, the solution of a linear system won't be accurate, since the small perturbations on $y$ introduced by the floating point system will be amplified and the corresponding solution will be drammatically distant to the true solution.\n",
    "\n",
    "How can we quantify the conditioning of a matrix? And how we define a matrix to be ill-conditioned?\n",
    "\n",
    "To quantify conditioning of an $n \\times n$ matrix $A$, it is common to consider the **condition number** which, whenever $A$ is invertible, is defined as:\n",
    "\n",
    "$$\n",
    "    k_p(A) = ||A||_p || A^{-1} ||_p\n",
    "$$\n",
    "\n",
    "Where $p \\geq 1$ idenfities the norm on which the condition number is computed. Due to the equivalence of norms, the **magnitude** of the condition number does not depend of $p$ (i.e. the magnitude of $k_p(A)$ should be approximately the same, whatever is the value of $p$). For this reason, it is typical to measure the condition number of a matrix by setting $p=2$.\n",
    "\n",
    "To compute the $p$-condition number of a matrix $A$ in `numpy`, you can use the function `np.linalg.cond(A, p)`.\n",
    "\n",
    "For example, let's check the condition number of the random matrix vs the condition number of the hilbert matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "\n",
    "# Setting up the dimension\n",
    "n = 15\n",
    "\n",
    "# Creating the test problem (with Hilbert matrix)\n",
    "A_random = np.random.randn(n, n) # n x n random matrix\n",
    "A_hilbert = scipy.linalg.hilbert(n) # n x n hilbert matrix\n",
    "\n",
    "print(f\"Cond. Number Random matrix: {np.linalg.cond(A_random, p=2)}\")\n",
    "print(f\"Cond. Number Hilbert matrix: {np.linalg.cond(A_hilbert, p=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the condition number of Hilbert matrix is insanely higher than the condition number of random matrix, which explains why the relative error while using the Hilbert matrix is higher than when using the random matrix.\n",
    "\n",
    "Now, where do we set the threshold on the condition number to define an operator to be **ill-conditioned** versus it being **well-conditioned**? Let's check an interesting property of the condition number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# We want to print out the condition number of the hilbert matrix A \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# for increasing dimension\u001b[39;00m\n\u001b[32m      3\u001b[39m n_max = \u001b[32m12\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m condition_numbers = \u001b[43mnp\u001b[49m.zeros((n_max, ))\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, n_max+\u001b[32m1\u001b[39m):\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Define the hilbert matrix\u001b[39;00m\n\u001b[32m      8\u001b[39m     A = scipy.linalg.hilbert(n)\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# We want to print out the condition number of the hilbert matrix A \n",
    "# for increasing dimension\n",
    "n_max = 12\n",
    "\n",
    "condition_numbers = np.zeros((n_max, ))\n",
    "for n in range(1, n_max+1):\n",
    "    # Define the hilbert matrix\n",
    "    A = scipy.linalg.hilbert(n)\n",
    "\n",
    "    # Compute the condition number\n",
    "    cond = np.linalg.cond(A, p=2)\n",
    "\n",
    "    # Print and save\n",
    "    print(f\"Condition number for n = {n}: {cond}.\")\n",
    "    condition_numbers[n-1] = cond # \"n-1\" because range begins by 1!\n",
    "\n",
    "# Plot the condition number in semilogy plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(1, n_max+1), condition_numbers)\n",
    "plt.grid()\n",
    "plt.xlabel(r\"$n$\")\n",
    "plt.ylabel(r\"$k_2(A)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the condition number starts relatively small, then grows exponentially for increasing values of $n$.\n",
    "\n",
    "This is not a coincidence. Indeed, the condition number of basically every matrix **grows with the dimensionality**! As a consequence, the threshold defining an ill-conditioned matrix should depend on $n$.\n",
    "\n",
    "This observation justifies the following definition:\n",
    "\n",
    "> An invertible matrix $A$ is said to be **ill-conditioned** if its condition number grows exponentially with the dimension of the problem, $n$, i.e. if $k_2(A) \\approx c10^{n}$ for a positive constant $c>0$.\n",
    "\n",
    "While:\n",
    "\n",
    "> An invertible matrix $A$ is said to be **well-conditioned** if its condition number grows linearly with the dimension of the problem, $n$, i.e. if $k_2(A) \\approx cn$ for a positive constant $c>0$.\n",
    "\n",
    "This can be checked by simply plotting the behavior of the condition number in `semilogy` scale. If the plot appears to be a straight line, then $A$ is ill-conditioned, otherwise, it is well-conditioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# We want to print out the condition number of the hilbert matrix vs random matrix\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# for increasing dimension\u001b[39;00m\n\u001b[32m      3\u001b[39m n_max = \u001b[32m12\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m condition_numbers_rand = \u001b[43mnp\u001b[49m.zeros((n_max, ))\n\u001b[32m      6\u001b[39m condition_numbers_hilb = np.zeros((n_max, ))\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, n_max+\u001b[32m1\u001b[39m):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Define the hilbert matrix\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# We want to print out the condition number of the hilbert matrix vs random matrix\n",
    "# for increasing dimension\n",
    "n_max = 12\n",
    "\n",
    "condition_numbers_rand = np.zeros((n_max, ))\n",
    "condition_numbers_hilb = np.zeros((n_max, ))\n",
    "for n in range(1, n_max+1):\n",
    "    # Define the hilbert matrix\n",
    "    A_rand = np.random.randn(n, n)\n",
    "    A_hilb = scipy.linalg.hilbert(n)\n",
    "\n",
    "    # Compute the condition number\n",
    "    cond_rand = np.linalg.cond(A_rand, p=2)\n",
    "    cond_hilb = np.linalg.cond(A_hilb, p=2)\n",
    "\n",
    "    # Print and save\n",
    "    condition_numbers_rand[n-1] = cond_rand # \"n-1\" because range begins by 1!\n",
    "    condition_numbers_hilb[n-1] = cond_hilb # \"n-1\" because range begins by 1!\n",
    "\n",
    "# Plot the condition number in semilogy plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(np.arange(1, n_max+1), condition_numbers_rand)\n",
    "plt.semilogy(np.arange(1, n_max+1), condition_numbers_hilb)\n",
    "plt.grid()\n",
    "plt.xlabel(r\"$n$\")\n",
    "plt.ylabel(r\"$\\log k_2(A)$\")\n",
    "plt.legend([\"Random\", \"Hilbert\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we relate the condition number of a matrix with the accuracy of the solution to a linear system associated with it?\n",
    "\n",
    "Consider the following inequality:\n",
    "\n",
    "$$\n",
    "    \\frac{|| \\delta x ||}{||x||} \\leq k_2(A) \\Bigl( \\frac{||\\delta A||}{|| A ||} + \\frac{|| \\delta y ||}{|| y ||} \\Bigr).\n",
    "$$\n",
    "\n",
    "It implies that the relative error on the computed solution is big whenever $k_2(A)$ is big. Moreover, note that as a consequence of the formula above, the accuracy of a computed solution is partially a proprierty of the condition number of $A$ itself, meaning that (again) **no algorithm** is able to compute an accurate solution to an ill-conditioned system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Linear System by Matrix Splitting\n",
    "As you should know, when the matrix $A$ is unstructured, the linear system $Ax = y$ can be efficiently solved by using [LU Decomposition](https://en.wikipedia.org/wiki/LU_decomposition). In particular, with Gaussian elimination algorithm, one can factorize any non-singular matrix $A \\in \\mathbb{R}^{n \\times n}$ into:\n",
    "\n",
    "$$\n",
    "    A = PLU\n",
    "$$\n",
    "\n",
    "where $L \\in \\mathbb{R}^{n \\times n}$ is a lower-triangular matrix, $U \\in \\mathbb{R}^{n \\times n}$ is an upper-triangular matrix with all ones on the diagonal and $P \\in \\mathbb{R}^{n \\times n}$ is a permutation matrix (i.e. a matrix obtained by permutating the rows of the identity matrix). If the decomposition is computed without pivoting, the permutation matrix equals the identity. Note that the assumption that $A$ is non-singular is not restrictive, since it is a necessary condition for the solvability of $Ax = y$. \n",
    "\n",
    "Since $P$ is an orthogonal matrix, $P^{-1} = P^T$, thus\n",
    "\n",
    "$$\n",
    "    A = PLU \\iff P^T A = LU\n",
    "$$\n",
    "\n",
    "Since linear systems of the form \n",
    "\n",
    "$$\n",
    "    Lx = y \\quad \\text{ and } \\quad Ux = y\n",
    "$$\n",
    "\n",
    "can be efficiently solved by the Forward (Backward) substitution, and the computation of the LU factorization by Gaussian elimination is pretty fast ($O(n^3)$ floating point operations), we can use that to solve the former linear system. \n",
    "\n",
    "Indeed,\n",
    "\n",
    "$$\n",
    "    Ax = y \\iff P^TAx = P^Ty \\iff LUx = P^Ty\n",
    "$$\n",
    "\n",
    "then, by Forward-Backward substitution, this system can be solved by subsequently solve \n",
    "\n",
    "$$\n",
    "    Lz = P^Ty \\quad \\text{ then } \\quad Ux = z\n",
    "$$\n",
    "\n",
    "whose solution is a solution for $Ax = y$.\n",
    "\n",
    "Even if this procedure is automatically performed by the `np.linalg.solve` function, we can unroll it with the functions `scipy.linalg.lu(A)` and `scipy.linalg.solve_triangular(A, b)`, whose documentation can be found [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu.html) and [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve_triangular.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Exercise:_** Write a function that takes as input a non-singular matrix $A \\in \\mathbb{R}^{n \\times n}$ and a vector $y \\in \\mathbb{R}^n$ and returns the solution $x \\in \\mathbb{R}^n$ of $Ax = y$, with the `np.linalg.solve` and the `scipy.linalg.lu()` + `scipy.linalg.solve_triangular()` functions. Compare the time required for the two algorithms when $A$ is a random matrix of dimension $n = 1000$, while $y$ is the datum built such that $x_{true}$ is the vector of all ones. Check the reconstruction errors in the two cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cholesky factorization\n",
    "If the matrix $A$ is SDP (symmetric and positive definite), then we can halve the number of operations required to compute the solution to the system by considering the Cholesky decomposition, which factorizes $AA$ as:\n",
    "\n",
    "$$\n",
    "A = L L^T\n",
    "$$\n",
    "\n",
    "where $L$ is a non-singular lower-triangular matrix. Then, we can simply use the forward-backward algorithm to solve the system $Ax = y$. \n",
    "\n",
    "The Cholesky decomposition can be computed in Python with the command `np.linalg.cholesky`.\n",
    "\n",
    "Since it holds the following result:\n",
    "\n",
    "> **Theorem:** A square matrix $A$ is SDP **if and only if** it admits a Cholesky decomposition,\n",
    "\n",
    "the function `np.linalg.cholesky` can be also used to check if a matrix $A$ is SDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Repeat the same exericise as before, setting $A$ to be the Hilbert matrix, $n=10$, $x_{true}$ is the vector of all ones. Check the accuracy and the time required by computing the solution with:\n",
    "> - The built-in `numpy` function.\n",
    "> - The LU decomposition (with `scipy`) with `solve_triangular`.\n",
    "> - The Cholesky decomposition with `solve_triangular`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}