
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Unsupervised Learning via Linear Algebra &#8212; Statistical and Mathematical Methods for Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Optimization/unsupervised';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Homework 1: Gradient Descent" href="../Homeworks/HW1.html" />
    <link rel="prev" title="Supervised Learning for Classification" href="supervised.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Statistical and Mathematical Methods for Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Statistical and Mathematical Methods for Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Statistical and Mathematical Methods for Machine Learning (SMM)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">NLA with Python</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../NLA_numpy/basics_python.html">Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../NLA_numpy/introduction_to_numpy.html">Introduction to Python for NLA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../NLA_numpy/matplotlib.html">Visualization with Matplotlib</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro_ML.html">A (Very Short) Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="GD.html">Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="SGD.html">Stochastic Gradient Descent (SGD)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="supervised.html">Supervised Learning for Classification</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Unsupervised Learning via Linear Algebra</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Homeworks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Homeworks/HW1.html">Homework 1: Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Homeworks/HW2.html">Homework 2: Stochastic Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Homeworks/HW3.html">Homework 3: Supervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Homeworks/HW4.html">Homework 4: Unsupervised Learning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/devangelista2/statistical-mathematical-methods" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/devangelista2/statistical-mathematical-methods/issues/new?title=Issue%20on%20page%20%2FOptimization/unsupervised.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Optimization/unsupervised.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Unsupervised Learning via Linear Algebra</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-singular-value-decomposition-svd">The Singular Value Decomposition (SVD)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svd-of-a-matrix-in-python">SVD of a Matrix in Python</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-numerical-rank-of-a">The numerical rank of <span class="math notranslate nohighlight">\(A\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#svd-for-image-compression">SVD for Image Compression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction-with-pca">Dimensionality Reduction with PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-example">Python example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-digits">Visualizing the digits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#filtering-digits">Filtering digits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#splitting-the-dataset">Splitting the dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-clusters">Visualizing clusters</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="unsupervised-learning-via-linear-algebra">
<h1>Unsupervised Learning via Linear Algebra<a class="headerlink" href="#unsupervised-learning-via-linear-algebra" title="Link to this heading">#</a></h1>
<p>In the previous chapters, our learning paradigm relied on <strong>supervision</strong>: we were provided with input–output pairs <span class="math notranslate nohighlight">\((X, Y)\)</span>, and the goal was to <em>learn</em> a function that maps each input to the correct output. We developed models such as linear regression, logistic regression and simple neural networks, and trained them using iterative <strong>gradient-based optimization</strong> methods like <em>SGD</em> and <em>Adam</em>. In those settings, the loss function was explicitly defined by comparing predictions with ground-truth labels, and learning meant progressively reducing this discrepancy.</p>
<p>We now turn to a different and equally fundamental perspective: <strong>unsupervised learning</strong>. In this setting, we are given only the data <span class="math notranslate nohighlight">\(X\)</span>, without any accompanying target values <span class="math notranslate nohighlight">\(Y\)</span>. The task is to discover <strong>structure</strong> hidden in the data such as patterns, groupings, low-dimensional representations, or meaningful features that are not explicitly provided. Instead of learning “from answers”, we now try to learn <strong>from the data itself</strong>.</p>
<p>Unsupervised learning plays a crucial role in modern machine learning and data analysis. Two of its most impactful areas are:</p>
<ul class="simple">
<li><p><strong>Dimensionality reduction / data compression</strong>: real-world datasets often live in high-dimensional spaces, but much of that dimensionality is redundant. By finding compact, informative representations, we can improve storage, visualization, and even performance of supervised models.<br />
<em>Example: reducing the number of features needed to describe images, sound recordings, or text embeddings.</em></p></li>
<li><p><strong>Clustering</strong>: identifying groups or categories within data <em>without</em> labels.<br />
<em>Example: segmenting clients by purchasing behavior, grouping similar images without annotations, recognizing patterns in biological or sensor data.</em></p></li>
</ul>
<p>In contrast to neural network training, where gradients and learning rates play a central role, many powerful unsupervised methods are based entirely on <strong>linear algebra</strong>. A particularly elegant tool in this direction is the <strong>Singular Value Decomposition (SVD)</strong>, a fundamental matrix factorization technique that reveals geometric and statistical structure in the data. With SVD, we can:</p>
<ul class="simple">
<li><p>discover the principal directions of variation in the data,</p></li>
<li><p>build optimal low-rank approximations (for compression),</p></li>
<li><p>and apply those ideas to perform clustering based on latent structure.</p></li>
</ul>
<p>In this chapter, we will study how unsupervised learning can be achieved <strong>without gradients</strong>, using only matrix decompositions and linear algebraic reasoning. You will see that, although philosophically different from the supervised techniques studied so far, these methods share the same motivation: extract meaningful information from data. The only difference, compared to supervised learning, is that now we aim to do that <strong>without ever knowing the “correct answers” in advance</strong>.</p>
<p>We begin by introducing SVD, understanding how it relates to variance maximization and optimal projections, and then show how it can be used for dimensionality reduction and clustering.</p>
<section id="the-singular-value-decomposition-svd">
<h2>The Singular Value Decomposition (SVD)<a class="headerlink" href="#the-singular-value-decomposition-svd" title="Link to this heading">#</a></h2>
<p>In Data Analysis, it is often required to compress the data, either to make it more manageable or to be able to visualize the most important features, reducing redundancy. The two tasks, usually named <strong>data compression</strong> and <strong>dimensionality reduction</strong>, are mathematically equivalent and closely related to the concept of <strong>Singular Value Decomposition (SVD)</strong> of a matrix.</p>
<p>Recall that:</p>
<blockquote>
<div><p>An invertible matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span> is said to be <strong>orthogonal</strong> if <span class="math notranslate nohighlight">\(A^T A = I\)</span> or, equivalently, <span class="math notranslate nohighlight">\(A^T = A^{-1}\)</span>.</p>
</div></blockquote>
<p>Now, consider a given matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span>. It can be proved that it can be <strong>always</strong> factorized into the product of three matrices,</p>
<div class="math notranslate nohighlight">
\[
A = U \Sigma V^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(U \in \mathbb{R}^{m \times m}\)</span> and <span class="math notranslate nohighlight">\(V \in \mathbb{R}^{n \times n}\)</span> are orthogonal matrices, while <span class="math notranslate nohighlight">\(\Sigma \in \mathbb{R}^{m \times n}\)</span> is a rectangular matrix which is non-zero only on the diagonal. Such decomposition is named <strong>Singular Value Decomposition (SVD)</strong> of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>Of particular interest in our analysis are the values on the diagonal of <span class="math notranslate nohighlight">\(\Sigma\)</span>, named <strong>singular values</strong> of <span class="math notranslate nohighlight">\(A\)</span>, and usually denoted as <span class="math notranslate nohighlight">\(\sigma_1, \dots, \sigma_{\min \{ m, n \}}\)</span>. In particular, it is known that the singular values:</p>
<ul class="simple">
<li><p>are always greater or equal to 0, i.e. <span class="math notranslate nohighlight">\(\sigma_i \geq 0\)</span>, <span class="math notranslate nohighlight">\(\forall i\)</span>;</p></li>
<li><p>are ordered in descending order, i.e. <span class="math notranslate nohighlight">\(\sigma_1 \geq \sigma_2 \geq \dots \geq 0\)</span>;</p></li>
<li><p>can be used to determine the rank of <span class="math notranslate nohighlight">\(A\)</span>, since it is equal to the number of singular values strictly greater than zero, i.e. if <span class="math notranslate nohighlight">\(\sigma_1 \geq \sigma_2 \geq \dots \sigma_r &gt; 0\)</span> and <span class="math notranslate nohighlight">\(\sigma_{r+1} = 0\)</span> for some index <span class="math notranslate nohighlight">\(r\)</span>, then <span class="math notranslate nohighlight">\(r = rk(A)\)</span>.</p></li>
</ul>
<p>A useful properties of the SVD of <span class="math notranslate nohighlight">\(A\)</span> is that it can be used to compress the informations contained in <span class="math notranslate nohighlight">\(A\)</span> itself. Indeed, note that the SVD decomposition allows to rewrite <span class="math notranslate nohighlight">\(A\)</span> as the sum of simple matrices, i.e.</p>
<div class="math notranslate nohighlight">
\[
A = U \Sigma V^T = \sum_{i=1}^r \sigma_i u_i v_i^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(u_i\)</span> and <span class="math notranslate nohighlight">\(v_i\)</span> are the columns of <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span>, respectively. Each term <span class="math notranslate nohighlight">\(u_i v_i^T\)</span> is a rank-1 matrix named <strong>dyad</strong>, and the <span class="math notranslate nohighlight">\(i\)</span>-th singular value <span class="math notranslate nohighlight">\(\sigma_i\)</span> represent the importance of the <span class="math notranslate nohighlight">\(i\)</span>-th dyad in the construction of <span class="math notranslate nohighlight">\(A\)</span>. In particular, the SVD decomposition allows to deconstruct <span class="math notranslate nohighlight">\(A\)</span> into the sum of matrices with decreasing information content.</p>
<p>The SVD decomposition can be used to compress the matrix <span class="math notranslate nohighlight">\(A\)</span> by considering its <span class="math notranslate nohighlight">\(k\)</span>-rank approximation <span class="math notranslate nohighlight">\(A_k\)</span>, defined as</p>
<div class="math notranslate nohighlight">
\[
A_k = \sum_{i=1}^k \sigma_i u_i v_i^T.
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <span class="math notranslate nohighlight">\(k\)</span>-rank approximation <span class="math notranslate nohighlight">\(A_k\)</span> of <span class="math notranslate nohighlight">\(A\)</span> is simply obtained by stopping the summation defining <span class="math notranslate nohighlight">\(A\)</span> through dyads at term <span class="math notranslate nohighlight">\(k \leq r\)</span>. Whenever <span class="math notranslate nohighlight">\(k = r\)</span>, clearly <span class="math notranslate nohighlight">\(A_k = A\)</span>.</p>
</div>
<p>It can also be shown that the <span class="math notranslate nohighlight">\(k\)</span>-rank approximation of <span class="math notranslate nohighlight">\(A\)</span> is the <span class="math notranslate nohighlight">\(k\)</span>-rank matrix that minimizes the distance (expressed in 2-norm) from <span class="math notranslate nohighlight">\(A\)</span>, i.e.</p>
<div class="math notranslate nohighlight">
\[
A_k = \arg\min_{M: rk(M) = k} || M - A ||_2.
\]</div>
<section id="svd-of-a-matrix-in-python">
<h3>SVD of a Matrix in Python<a class="headerlink" href="#svd-of-a-matrix-in-python" title="Link to this heading">#</a></h3>
<p>The SVD can be easily computed in Python using functions from the <code class="docutils literal notranslate"><span class="pre">numpy</span></code> package.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing numpy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Consider an example matrix</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Measure the shape of A: which is the maximum rank?</span>
<span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The shape of A is: </span><span class="si">{</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

<span class="c1"># Compute the SVD decomposition of A and check the shapes</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">U</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">VT</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Define the full matrix S</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">S</span><span class="p">[:</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The shape of A is: (10, 6).
(10, 10) (6,) (6, 6)
</pre></div>
</div>
</div>
</div>
<p>Here we computed the SVD of <code class="docutils literal notranslate"><span class="pre">A</span></code> through the function <code class="docutils literal notranslate"><span class="pre">np.linalg.svd()</span></code>, that takes as input a matrix and returns a triplet <code class="docutils literal notranslate"><span class="pre">U,</span> <span class="pre">s,</span> <span class="pre">VT</span></code>, representing the matrices <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(V^T\)</span> and a <strong>vectorized</strong> version of <span class="math notranslate nohighlight">\(\Sigma\)</span> that only contains the diagonal (to save memory!).</p>
<p>Note that, in some situations, it can be useful to compute the full matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>, as we did at the bottom of the above code snippet.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If the full matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> can be avoided, do not construct it explicitely.</p>
</div>
<blockquote>
<div><p><strong>Exercise:</strong> Verify that the above algorithm works as expected, by proving that <span class="math notranslate nohighlight">\( A \approx U \Sigma V^T \iff || A - U \Sigma V^T ||_2 \approx 0.\)</span></p>
</div></blockquote>
<blockquote>
<div><p><strong>Exercise:</strong> Compute the <span class="math notranslate nohighlight">\(k\)</span>-rank approximation <span class="math notranslate nohighlight">\(A_k\)</span> of <span class="math notranslate nohighlight">\(A\)</span>. Specifically, write a Python function that takes as input an integer <span class="math notranslate nohighlight">\(k \leq \min \{ m, n \}\)</span> and computes the <span class="math notranslate nohighlight">\(k\)</span>-rank approximation <span class="math notranslate nohighlight">\(A_k = \sum_{i=1}^k \sigma_i u_i v_i^T\)</span>. Then, test it on a matrix of your preference and compute the approximation error in Frobenius norm, <span class="math notranslate nohighlight">\(\| A - A_k \|_F\)</span>.</p>
</div></blockquote>
</section>
<section id="the-numerical-rank-of-a">
<h3>The numerical rank of <span class="math notranslate nohighlight">\(A\)</span><a class="headerlink" href="#the-numerical-rank-of-a" title="Link to this heading">#</a></h3>
<p>In theory, the rank of a matrix is the number of non-zero singular values in its Singular Value Decomposition (SVD). However, in real-world data (and floating-point arithmetic), singular values rarely vanish exactly; instead, many become <em>very small</em> due to noise, correlations, or computational precision. The <strong>numerical rank</strong> of a matrix refers to the number of singular values that are significantly larger than zero according to a chosen tolerance. In practice, it represents the <em>effective dimensionality</em> of the matrix: how many directions in space defined by <span class="math notranslate nohighlight">\(A\)</span> truly contain information. Small singular values are often associated with noise or redundant structure, while the dominant singular values capture meaningful patterns. Identifying the numerical rank is fundamental for tasks such as dimensionality reduction, compression, and noise filtering, as it tells us how many principal components we actually need to faithfully represent the dataset.</p>
<blockquote>
<div><p><strong>Exercise:</strong> Compute the rank of <span class="math notranslate nohighlight">\(A\)</span> by using the formula:
<span class="math notranslate nohighlight">\( rk(A) = r \text{ s.t. } \sigma_r &gt; 0, \sigma_{r+1} = 0\)</span>,
and compare it with the output of the built-in function in <code class="docutils literal notranslate"><span class="pre">numpy</span></code>.</p>
</div></blockquote>
</section>
</section>
<section id="svd-for-image-compression">
<h2>SVD for Image Compression<a class="headerlink" href="#svd-for-image-compression" title="Link to this heading">#</a></h2>
<p>From a computational point of view, a grey-scale image is a <strong>matrix</strong> with shape <code class="docutils literal notranslate"><span class="pre">(height,</span> <span class="pre">width)</span></code>, such that the element in position <span class="math notranslate nohighlight">\(i, j\)</span> contains the intensity of the pixel in the corresponding position. An RGB image is a triplet of matrices such that in position <span class="math notranslate nohighlight">\(i, j\)</span>, each of the three matrices represents the amount of Red, Green and Blue in the corresponding pixel.</p>
<p>To work with images, we consider the <code class="docutils literal notranslate"><span class="pre">skimage.data</span></code> submodule:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">skimage</span>

<span class="c1"># Loading the &quot;cameraman&quot; image</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">skimage</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">camera</span><span class="p">()</span>

<span class="c1"># Printing its shape</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape of the image: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of the image: (512, 512).
</pre></div>
</div>
</div>
</div>
<p>To visualize a matrix as an image, it can be used the <code class="docutils literal notranslate"><span class="pre">plt.imshow()</span></code> function from the <code class="docutils literal notranslate"><span class="pre">matplotlib.pyplot</span></code> module. If the image is a grey-scale image (as it is the case for the <code class="docutils literal notranslate"><span class="pre">camera</span></code> image we are considering), it is required to set the <code class="docutils literal notranslate"><span class="pre">cmap='gray'</span></code> option to visualize it as a grey-scale image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/50aff3d36362558deba307a37ae4682958e48d2b46c8fe49483f0733bf1123b0.png" src="../_images/50aff3d36362558deba307a37ae4682958e48d2b46c8fe49483f0733bf1123b0.png" />
</div>
</div>
<p>Besides the visualization, the image is still a matrix and all the techniques already studied for matrices can be used on images. In particular, the <span class="math notranslate nohighlight">\(k\)</span>-rank approximation can be used to compress it with minimal information loss.</p>
<p>We recall that the <span class="math notranslate nohighlight">\(k\)</span>-rank approximation <span class="math notranslate nohighlight">\(X_k\)</span> of the image <span class="math notranslate nohighlight">\(X\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
    X_k = \sum_{i=1}^k \sigma_i u_i v_i^T
\]</div>
<p>where each <span class="math notranslate nohighlight">\(\sigma_i\)</span> is a scalar number, <span class="math notranslate nohighlight">\(u_i\)</span> is an <span class="math notranslate nohighlight">\(m\)</span>-dimensional vector, while <span class="math notranslate nohighlight">\(v_i\)</span> is an <span class="math notranslate nohighlight">\(n\)</span>-dimensional vector. As a consequence, the number of values required to memorize <span class="math notranslate nohighlight">\(X_k\)</span> is <span class="math notranslate nohighlight">\(k(m + n + 1)\)</span>, while the number of values required to memorize the whole image <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\(mn\)</span>. As a consequence, the compression factor (i.e. the percentage of pixels we saved in memorizing <span class="math notranslate nohighlight">\(X_k\)</span> instead of <span class="math notranslate nohighlight">\(X\)</span>) can be computed as:</p>
<div class="math notranslate nohighlight">
\[
    c_k = 1 - \frac{k (m + n + 1)}{mn}.
\]</div>
<blockquote>
<div><p><strong>Exercise:</strong> Using the function defined in the previous exercise, compute the <span class="math notranslate nohighlight">\(k\)</span>-rank approximation of the cameraman image <span class="math notranslate nohighlight">\(X\)</span> for different values of <span class="math notranslate nohighlight">\(k\)</span> and observe the behavior of each reconstruction. Also, compute and plot the compression factor <span class="math notranslate nohighlight">\(c_k\)</span> for each value of <span class="math notranslate nohighlight">\(k\)</span>.</p>
</div></blockquote>
</section>
<section id="dimensionality-reduction-with-pca">
<h2>Dimensionality Reduction with PCA<a class="headerlink" href="#dimensionality-reduction-with-pca" title="Link to this heading">#</a></h2>
<p>Working with data, it is common to have access to very high-dimensional unstructured informations (e.g. images, sounds, …). To effectively work with them, it is necessary to find a way to project them into a low-dimensional space where data <strong>semantically similar</strong> is <strong>close</strong>. This approach is called <strong>dimensionality reduction</strong>.</p>
<p>For example, assume our data can be stored in an <span class="math notranslate nohighlight">\(N \times d\)</span> array,</p>
<div class="math notranslate nohighlight">
\[
    X = [ x^{(1)}; x^{(2)}; \dots x^{(N)} ] \in \mathbb{R}^{N \times d}
\]</div>
<p>where each datapoint <span class="math notranslate nohighlight">\(x^{(j)} \in \mathbb{R}^d\)</span>. The idea of dimensionality reduction techniques in ML is to find a projector operator <span class="math notranslate nohighlight">\(P: \mathbb{R}^d \to \mathbb{R}^k\)</span>, with <span class="math notranslate nohighlight">\(k \ll d\)</span>, such that in the projected space <span class="math notranslate nohighlight">\(P(x)\)</span>, images semantically similar are close together. If the points in a projected space forms isolated popoulations such that <em>inside</em> of each popoulation the points are close, while the distance <em>between</em> popoulations is large, we call them <strong>clusters</strong>. A clusering algorithm is an algorithm which is able to find clusters from high-dimensional data.</p>
<section id="principal-component-analysis-pca">
<h3>Principal Component Analysis (PCA)<a class="headerlink" href="#principal-component-analysis-pca" title="Link to this heading">#</a></h3>
<p>Principal Componenti Analyisis (PCA) is arguably the simplest yet effective technique to perform dimensionality reduction and clustering. It is an unsupervised algorithm, thus it does not require any label. The idea is the following: consider a dataset <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{N \times d}\)</span> of high-dimensional data and assume we want to project it into a low-dimensional space <span class="math notranslate nohighlight">\(\mathbb{R}^k\)</span>. Define:</p>
<div class="math notranslate nohighlight">
\[
    Z = [z^{(1)}; z^{(2)}; \dots z^{(N)}] \in \mathbb{R}^{N \times k},
\]</div>
<p>the projected version of <span class="math notranslate nohighlight">\(X\)</span>. We want to find a matrix <span class="math notranslate nohighlight">\(P \in \mathbb{R}^{k \times d}\)</span> such that <span class="math notranslate nohighlight">\(Z = XP^T\)</span>, with the constraint that in the projected space we want to keep as much information as possible from the original data <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>You already studied that, when you want to project a matrix by keeping informations, a good idea is to use the Singular Value Decomposition (SVD) of it and, in particular, the Truncated SVD (TSVD). Let <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{N \times d}\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
X = U \Sigma V^T
\]</div>
<p>is the SVD of <span class="math notranslate nohighlight">\(X\)</span>, where <span class="math notranslate nohighlight">\(U \in \mathbb{R}^{N \times N}\)</span>, <span class="math notranslate nohighlight">\(V \in \mathbb{R}^{d \times d}\)</span> are orthogonal matrices (<span class="math notranslate nohighlight">\(U^T U = U U^T = I\)</span> and <span class="math notranslate nohighlight">\(V V^T = V^T V = I\)</span>), while <span class="math notranslate nohighlight">\(\Sigma \in \mathbb{R}^{N \times d}\)</span> is a diagonal matrix whose diagonal elements <span class="math notranslate nohighlight">\(\sigma_i\)</span> are the singular values of <span class="math notranslate nohighlight">\(X\)</span>, in decreasing order (<span class="math notranslate nohighlight">\(\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_d\)</span>). Since the singular values represents the <em>quantity of informations</em> contained in the corresponding singular vectors, keeping the first <span class="math notranslate nohighlight">\(k\)</span> singular values and vectors can be the solution to our projection problem. Indeed, given <span class="math notranslate nohighlight">\(k &lt; d\)</span>, we define the Truncated SVD of <span class="math notranslate nohighlight">\(X\)</span> as</p>
<div class="math notranslate nohighlight">
\[
X_k = U_k \Sigma_k V^T_k
\]</div>
<p>where <span class="math notranslate nohighlight">\(U_k \in \mathbb{R}^{N \times k}\)</span>, <span class="math notranslate nohighlight">\(\Sigma_k \in \mathbb{R}^{k \times k}\)</span>, and <span class="math notranslate nohighlight">\(V_k \in \mathbb{R}^{d \times k}\)</span>.</p>
<p>The PCA use this idea and defines the projection matrix as <span class="math notranslate nohighlight">\(P = V_k^T\)</span>, and consequently,</p>
<div class="math notranslate nohighlight">
\[
    Z = X P^T := X V_k
\]</div>
<p>is the projected space. Here, the columns of <span class="math notranslate nohighlight">\(V_k\)</span> are called <strong>feature vectors</strong>, while the columns of <span class="math notranslate nohighlight">\(Z\)</span> are the <strong>principal components</strong> of <span class="math notranslate nohighlight">\(X\)</span>.</p>
</section>
<section id="implementation">
<h3>Implementation<a class="headerlink" href="#implementation" title="Link to this heading">#</a></h3>
<p>To implement PCA, a first step is to <em>center</em> the data. This can be done by computing its centroid <span class="math notranslate nohighlight">\(c(X)\)</span> which is then subtracted to <span class="math notranslate nohighlight">\(X\)</span> to obtain:</p>
<div class="math notranslate nohighlight">
\[
X_c = X - c(X),
\]</div>
<p>which has the important property that <span class="math notranslate nohighlight">\(c(X_c) = 0\)</span>.</p>
<blockquote>
<div><p>Given a set <span class="math notranslate nohighlight">\(X = [x^{(1)}; x^{(2)}; \dots x^{(N)}]\)</span>, its <strong>centroid</strong> is defined as <span class="math notranslate nohighlight">\(c(X) = \frac{1}{N} \sum_{i=1}^N x^{(i)}\)</span>.</p>
</div></blockquote>
<p>The implementation of PCA thus read as follows:</p>
<ul class="simple">
<li><p>Consider the dataset <span class="math notranslate nohighlight">\(X\)</span>;</p></li>
<li><p>Compute the centered version of <span class="math notranslate nohighlight">\(X\)</span> as <span class="math notranslate nohighlight">\(X_c = X - c(X)\)</span>, where the subtraction between matrix and vector is executed <em>row-by-row</em>;</p></li>
<li><p>Compute the SVD of <span class="math notranslate nohighlight">\(X_c\)</span>, <span class="math notranslate nohighlight">\(X_c = U\Sigma V^T\)</span>;</p></li>
<li><p>Given <span class="math notranslate nohighlight">\(k &lt; n\)</span>, compute the Truncated SVD of <span class="math notranslate nohighlight">\(X_c\)</span>: <span class="math notranslate nohighlight">\(X_{c, k} = U_k \Sigma_k V_k^T\)</span>;</p></li>
<li><p>Compute the projected dataset <span class="math notranslate nohighlight">\(Z_k = X_c V_k\)</span>;</p></li>
</ul>
</section>
<section id="python-example">
<h3>Python example<a class="headerlink" href="#python-example" title="Link to this heading">#</a></h3>
<p>In the following, we consider as an example the MNIST dataset, which can be download from Kaggle (<a class="reference external" href="https://www.kaggle.com/datasets/animatronbot/mnist-digit-recognizer">kaggle.com/datasets/animatronbot/mnist-digit-recognizer</a>). For simplicity, I renamed it as <code class="docutils literal notranslate"><span class="pre">MNIST.csv</span></code> and I placed it a folder named <code class="docutils literal notranslate"><span class="pre">data</span></code> into the current project folder.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Due to <code class="docutils literal notranslate"><span class="pre">github</span></code> limitations, in the following we consider a smaller version of the data with size 12486 instead of 42000. However, we keep the discussion as if the full dataset is used, as this is expected when using the MNIST dataset.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Load data into memory (we use numpy as pandas may not work on Lab PCs)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;data/MNIST.csv&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape of the data: </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of the data: (12486, 785)
</pre></div>
</div>
</div>
</div>
<p>Note that our dataset is a <span class="math notranslate nohighlight">\(42000 \times 785\)</span> frame, where the columns <strong>from the second to the last are the pixels</strong> of an image representing an handwritten digit, while the <strong>first column</strong> is the <em>target</em>, i.e. the integer describing the represented digit.</p>
<p>Let’s therefore split this data into input data <span class="math notranslate nohighlight">\(X\)</span> and target data <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split data into a matrix X and a vector Y where:</span>
<span class="c1">#</span>
<span class="c1"># X is dimension (42000, 784)</span>
<span class="c1"># Y is dimension (42000, )</span>
<span class="c1"># Y is the first column of data, while X is everything else</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape of X: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, Shape of Y:</span><span class="si">{</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of X: (12486, 784), Shape of Y:(12486,).
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualizing-the-digits">
<h3>Visualizing the digits<a class="headerlink" href="#visualizing-the-digits" title="Link to this heading">#</a></h3>
<p>We already observed that <span class="math notranslate nohighlight">\(X\)</span> is a dataset of images representing handwritten digits. To visualize them, note that in the documentation, we can read that each datapoint is a <span class="math notranslate nohighlight">\(28 \times 28\)</span> grey-scale image, which has been <strong>flattened</strong>. Flattening is the operation of taking a 2-dimensional array (a matrix) and converting it to a 1-dimensional array, by concatenating the rows of it. This can be implemented in <code class="docutils literal notranslate"><span class="pre">numpy</span></code> with the function <code class="docutils literal notranslate"><span class="pre">a.flatten()</span></code>, where <code class="docutils literal notranslate"><span class="pre">a</span></code> is a 2-dimensional numpy array.</p>
<p>Since we know that the dimension of each image was <span class="math notranslate nohighlight">\(28 \times 28\)</span> before flattening, we can invert this procedure by <strong>reshaping</strong> them. After that, we can simply visualize it with the function <code class="docutils literal notranslate"><span class="pre">plt.imshow()</span></code> from <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>, by setting the <code class="docutils literal notranslate"><span class="pre">cmap</span></code> to <code class="docutils literal notranslate"><span class="pre">'gray'</span></code> since the images are grey-scale.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">visualize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="c1"># Visualize the image of index &#39;idx&#39; from the dataset &#39;X&#39;</span>

    <span class="c1"># Load an image in memory</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    
    <span class="c1"># Reshape it</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>

    <span class="c1"># Visualize</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Visualize image number 10 and the corresponding digit.</span>
<span class="n">idx</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">visualize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The associated digit is: </span><span class="si">{</span><span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8fa5363bcbd101ed7a83147743cb51eedd94da06cf805a6901a2508efe421811.png" src="../_images/8fa5363bcbd101ed7a83147743cb51eedd94da06cf805a6901a2508efe421811.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The associated digit is: 3.0
</pre></div>
</div>
</div>
</div>
</section>
<section id="filtering-digits">
<h3>Filtering digits<a class="headerlink" href="#filtering-digits" title="Link to this heading">#</a></h3>
<p>To simplify the dimensionality reduction task provided by PCA, we can consider filtering out just a few digits from MNIST dataset. This can be simply done by defining a boolean <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> whose value in position <span class="math notranslate nohighlight">\(i\)</span> is <code class="docutils literal notranslate"><span class="pre">True</span></code> if and only if the digit in the corresponding index represents one of the selected digit.</p>
<p>In the following, we show how this can be implemented for the digits <span class="math notranslate nohighlight">\(3\)</span> and <span class="math notranslate nohighlight">\(4\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the boolean array to filter out digits</span>
<span class="n">filter_3or4</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y</span><span class="o">==</span><span class="mi">3</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">Y</span><span class="o">==</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Define the filtered data</span>
<span class="n">X_filtered</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">filter_3or4</span><span class="p">]</span>
<span class="n">Y_filtered</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">filter_3or4</span><span class="p">]</span>

<span class="c1"># Print final shape of data</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final shape of X: </span><span class="si">{</span><span class="n">X_filtered</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">. Final shape of Y: </span><span class="si">{</span><span class="n">Y_filtered</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Memorize the shape of X in the variables N and d</span>
<span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">X_filtered</span><span class="o">.</span><span class="n">shape</span>

<span class="c1"># Visualize a sample just to prove that we only have 3 or 4</span>
<span class="n">visualize</span><span class="p">(</span><span class="n">X_filtered</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Final shape of X: (8423, 784). Final shape of Y: (8423,)
</pre></div>
</div>
<img alt="../_images/9ae5c906da107a65d15985f1e0a88243e686f7e681edb941644996d41b3ec515.png" src="../_images/9ae5c906da107a65d15985f1e0a88243e686f7e681edb941644996d41b3ec515.png" />
</div>
</div>
</section>
<section id="splitting-the-dataset">
<h3>Splitting the dataset<a class="headerlink" href="#splitting-the-dataset" title="Link to this heading">#</a></h3>
<p>Before implementing the algorithm performing PCA, you should <strong>always</strong> split the dataset into training and test set. Remember that to avoid any possible bias in the splitted dataset, the division should be done <strong>randomly</strong>, keeping a proportion of <em>at least</em> <span class="math notranslate nohighlight">\(80\%\)</span> for the training set and <span class="math notranslate nohighlight">\(20\%\)</span> for the test set.</p>
<p>The algorithm we consider to split the data is conceptually very similar to the one we used for sample batches in Stochastic Gradient Descent.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">split_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">N_train</span><span class="p">):</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Sample indices and shuffle them</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>

    <span class="c1"># Extract the set of index </span>
    <span class="n">train_idx</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:</span><span class="n">N_train</span><span class="p">]</span> <span class="c1"># for the training set</span>
    <span class="n">test_idx</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[</span><span class="n">N_train</span><span class="p">:]</span>  <span class="c1"># and for the test set</span>
    
    <span class="c1"># Training split</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
    
    <span class="c1"># Test split</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span>

<span class="c1"># Split</span>
<span class="n">N_train</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">split_data</span><span class="p">(</span><span class="n">X_filtered</span><span class="p">,</span> <span class="n">Y_filtered</span><span class="p">,</span> <span class="n">N_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(6738, 784) (1685, 784)
</pre></div>
</div>
</div>
</div>
<p>Now the dataset <span class="math notranslate nohighlight">\((X, Y)\)</span> is divided into the train and test components, and we can implement the PCA algorithm on <span class="math notranslate nohighlight">\(X_{train}\)</span>. <strong>Remember to never access <span class="math notranslate nohighlight">\(X_{test}\)</span> during training.</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute centroid</span>
<span class="n">cX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Make it a row vector</span>
<span class="n">cX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">cX</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape of c(X): </span><span class="si">{</span><span class="n">cX</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

<span class="c1"># Center the data (row-by-row)</span>
<span class="n">Xc</span> <span class="o">=</span> <span class="n">X_train</span> <span class="o">-</span> <span class="n">cX</span>

<span class="c1"># Compute SVD decomposition</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">Xc</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Given k, compute reduced SVD</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">VTk</span> <span class="o">=</span> <span class="n">VT</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span>

<span class="c1"># Define projection matrix</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">VTk</span>

<span class="c1"># Project X_train -&gt; Z_train</span>
<span class="n">Z_train</span> <span class="o">=</span> <span class="n">Xc</span> <span class="o">@</span> <span class="n">P</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape of projected data: </span><span class="si">{</span><span class="n">Z_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of c(X): (1, 784).
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of projected data: (6738, 2).
</pre></div>
</div>
</div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When the full SVD decomposition is not required (as it is the case for PCA), one can compute the (reduced) SVD decomposition of a matrix <span class="math notranslate nohighlight">\(X\)</span> as <code class="docutils literal notranslate"><span class="pre">np.linalg.svd(X,</span> <span class="pre">full_matrices=False)</span></code>, to save computation time.</p>
</div>
</section>
<section id="visualizing-clusters">
<h3>Visualizing clusters<a class="headerlink" href="#visualizing-clusters" title="Link to this heading">#</a></h3>
<p>When <span class="math notranslate nohighlight">\(k=2\)</span>, it is possible to visualize clusters in Python. In particular, we want to plot the datapoints, with the color of the corresponding class, to check how well the clustering algorithm performed in 2-dimensions. This can be done by the <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> function <code class="docutils literal notranslate"><span class="pre">plt.scatter</span></code>. In particular, if <span class="math notranslate nohighlight">\(Z_{train} = [z^{(1)}; z^{(2)}; \dots z^{(N)}] \in \mathbb{R}^{N \times 2}\)</span> is the projected dataset and <span class="math notranslate nohighlight">\(Y_{train} \in \mathbb{R}^N\)</span> is the vector of the corresponding classes, then the <span class="math notranslate nohighlight">\(Z_{train}\)</span> can be visualized as:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the clusters</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Z_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Z_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="o">*</span><span class="n">ax</span><span class="o">.</span><span class="n">legend_elements</span><span class="p">(),</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Digit&quot;</span><span class="p">)</span> <span class="c1"># Add to the legend the list of digits</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$z_1$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$z_2$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;PCA projection of MNIST digits 3 and 4&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/54a7dd7e155c179a0a71f230c05fd2d52391aafeaa0f216faa648c455476e5e2.png" src="../_images/54a7dd7e155c179a0a71f230c05fd2d52391aafeaa0f216faa648c455476e5e2.png" />
</div>
</div>
<blockquote>
<div><p><strong>Exercise:</strong> Try different combination of digits other than 3 or 4. What do you observe comparing the separation of the clusters and the similarity of the digits?</p>
</div></blockquote>
<blockquote>
<div><p><strong>Exercise:</strong> Keeping the projection matrix <span class="math notranslate nohighlight">\(P\)</span> <strong>fixed</strong>, try to project the test set <span class="math notranslate nohighlight">\(X_{test}\)</span> as <span class="math notranslate nohighlight">\(Z_{test} = X_{test} P^T\)</span>. Visualize <span class="math notranslate nohighlight">\(Z_{test}\)</span> on the same scatterplot of the projection of the training set. What do you observe? What if the digits in <span class="math notranslate nohighlight">\(X_{test}\)</span> are different from the digits in <span class="math notranslate nohighlight">\(X_{train}\)</span>?</p>
</div></blockquote>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Optimization"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="supervised.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Supervised Learning for Classification</p>
      </div>
    </a>
    <a class="right-next"
       href="../Homeworks/HW1.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Homework 1: Gradient Descent</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-singular-value-decomposition-svd">The Singular Value Decomposition (SVD)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svd-of-a-matrix-in-python">SVD of a Matrix in Python</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-numerical-rank-of-a">The numerical rank of <span class="math notranslate nohighlight">\(A\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#svd-for-image-compression">SVD for Image Compression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction-with-pca">Dimensionality Reduction with PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-example">Python example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-digits">Visualizing the digits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#filtering-digits">Filtering digits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#splitting-the-dataset">Splitting the dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-clusters">Visualizing clusters</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Davide Evangelista
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>