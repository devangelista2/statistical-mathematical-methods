{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "\n",
    "When training a Machine Learning model, we typically deal with a dataset  \n",
    "\n",
    "$$\n",
    "(X, Y) = \\{ (x^{(i)}, y^{(i)}) \\}_{i=1}^N,\n",
    "$$\n",
    "\n",
    "and a **parametric model** $ f_\\Theta(x) $ whose parameters $\\Theta \\in \\mathbb{R}^d$ (also called **weights**) must be learned.  \n",
    "The **training phase** is formulated as an **optimization problem**, where we seek parameters $\\Theta$ such that\n",
    "\n",
    "$$\n",
    "f_\\Theta(x^{(i)}) \\approx y^{(i)} \\quad \\text{for all } i=1,\\dots,N.\n",
    "$$\n",
    "\n",
    "To quantify this approximation, we introduce a **loss function** $\\mathcal{L}(\\Theta; X, Y)$, which measures how far the model predictions are from the true targets.\n",
    "\n",
    "## Empirical Risk Minimization\n",
    "\n",
    "In most ML problems, the loss can be expressed as a sum (or average) over **sample-wise losses**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta; X, Y) = \\frac{1}{N} \\sum_{i=1}^N \\ell_i(\\Theta; x^{(i)}, y^{(i)}),\n",
    "$$\n",
    "\n",
    "where $\\ell_i$ quantifies the error for the $i$-th training sample.\n",
    "\n",
    "The training optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\Theta^* = \\arg\\min_\\Theta \\, \\mathcal{L}(\\Theta; X, Y)\n",
    "         = \\arg\\min_\\Theta \\, \\frac{1}{N}\\sum_{i=1}^N \\ell_i(\\Theta; x^{(i)}, y^{(i)}).\n",
    "$$\n",
    "\n",
    "\n",
    "The **Gradient Descent (GD)** method updates the parameters iteratively as:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\Theta^{(0)} \\in \\mathbb{R}^d, \n",
    "\\Theta^{(k+1)} = \\Theta^{(k)} - \\alpha_k \\, \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}; X, Y)\n",
    "= \\Theta^{(k)} - \\alpha_k \\, \\frac{1}{N}\\sum_{i=1}^N \\nabla_\\Theta \\ell_i(\\Theta^{(k)}; x^{(i)}, y^{(i)}).\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Here $\\alpha_k > 0$ denotes the **learning rate** (or step size).\n",
    "\n",
    "## Example: Mean Squared Error (MSE)\n",
    "\n",
    "A common loss in regression problems is the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta; X, Y) = \\frac{1}{N}\\sum_{i=1}^N (f_\\Theta(x^{(i)}) - y^{(i)})^2\n",
    "= \\sum_{i=1}^N \\underbrace{\\frac{1}{N}(f_\\Theta(x^{(i)}) - y^{(i)})^2}_{=: \\ell_i(\\Theta; x^{(i)}, y^{(i)})}.\n",
    "$$\n",
    "\n",
    "\n",
    "By the chain rule:\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\ell_i(\\Theta; x^{(i)}, y^{(i)}) \n",
    "= \\frac{2}{N}\\,(f_\\Theta(x^{(i)}) - y^{(i)}) \\, \\nabla_\\Theta f_\\Theta(x^{(i)}),\n",
    "$$\n",
    "\n",
    "so that:\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\mathcal{L}(\\Theta; X, Y) \n",
    "= \\frac{2}{N}\\sum_{i=1}^N (f_\\Theta(x^{(i)}) - y^{(i)})\\,\\nabla_\\Theta f_\\Theta(x^{(i)}).\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore, one iteration of Gradient Descent becomes:\n",
    "\n",
    "$$\n",
    "\\Theta^{(k+1)} = \\Theta^{(k)} - \\alpha_k \\frac{2}{N}\\sum_{i=1}^N (f_\\Theta(x^{(i)}) - y^{(i)})\\,\\nabla_\\Theta f_\\Theta(x^{(i)}).\n",
    "$$\n",
    "\n",
    "\n",
    "## Motivation for Stochastic Gradient Descent\n",
    "\n",
    "While the computation of each $\\nabla_\\Theta \\ell_i(\\Theta; x^{(i)}, y^{(i)})$ is inexpensive,  \n",
    "the full gradient $\\nabla_\\Theta \\mathcal{L}(\\Theta; X, Y)$ requires summing over all $N$ samples â€” a prohibitive cost when $N$ is very large.\n",
    "\n",
    "The **Stochastic Gradient Descent (SGD)** algorithm addresses this by replacing the **exact gradient** with a **stochastic approximation** computed on a random subset of data (a *mini-batch*).\n",
    "\n",
    "## The SGD Algorithm\n",
    "\n",
    "Let $N_\\text{batch} \\ll N$ be the **batch size**, and let $\\mathcal{M}_k \\subset \\{1,\\dots,N\\}$ denote a random subset of indices such that $|\\mathcal{M}_k| = N_\\text{batch}$.\n",
    "\n",
    "At each iteration $k$:\n",
    "\n",
    "1. **Sample a mini-batch:**  \n",
    "   Randomly select a subset $\\mathcal{M}_k$ from the dataset (typically without replacement).\n",
    "\n",
    "2. **Compute the approximate gradient:**  \n",
    "   \n",
    "   $$\n",
    "   \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}; \\mathcal{M}_k)\n",
    "   = \\frac{1}{N_\\text{batch}}\\sum_{i\\in\\mathcal{M}_k} \\nabla_\\Theta \\ell_i(\\Theta^{(k)}; x^{(i)}, y^{(i)}).\n",
    "   $$\n",
    "\n",
    "\n",
    "3. **Update the parameters:**\n",
    "   \n",
    "   $$\n",
    "   \\Theta^{(k+1)} = \\Theta^{(k)} - \\alpha_k \\, \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}; \\mathcal{M}_k).\n",
    "   $$\n",
    "\n",
    "\n",
    "4. **Repeat** until all data have been used once.  \n",
    "   When the entire dataset has been processed, we say one **epoch** of SGD has been completed.  \n",
    "   The algorithm is typically run for a fixed number $E$ of epochs.\n",
    "\n",
    "## Comparison with Full Gradient Descent\n",
    "\n",
    "| Property | Gradient Descent (GD) | Stochastic Gradient Descent (SGD) |\n",
    "|-----------|------------------------|------------------------------------|\n",
    "| Gradient computation | Full dataset | Random subset (mini-batch) |\n",
    "| Iteration cost | High (depends on $N$) | Low (depends on $N_\\text{batch}$) |\n",
    "| Variance | Deterministic | Stochastic |\n",
    "| Convergence | Smooth and stable | Noisy, but often faster |\n",
    "| Scalability | Poor for large datasets | Excellent |\n",
    "\n",
    "The stochasticity introduces **noise** in the parameter updates, but this noise can help the optimizer **escape shallow local minima** and often improves generalization.\n",
    "\n",
    "## Typical Implementation\n",
    "\n",
    "Below is a reference Python-like pseudocode for SGD (for educational purposes only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def SGD(f, grad_f, X, Y, theta0, lr=1e-2, batch_size=32, epochs=10):\n",
    "    \"\"\"\n",
    "    Simplified Stochastic Gradient Descent (SGD) implementation.\n",
    "    f: loss function, grad_f: gradient wrt theta\n",
    "    X, Y: dataset\n",
    "    theta0: initial parameters\n",
    "    lr: learning rate\n",
    "    \"\"\"\n",
    "    theta = theta0.copy()\n",
    "    N = len(X)\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data indices\n",
    "        idx = np.random.permutation(N)\n",
    "        for start in range(0, N, batch_size):\n",
    "            batch_idx = idx[start:start+batch_size]\n",
    "            grad = grad_f(theta, X[batch_idx], Y[batch_idx])\n",
    "            theta -= lr * grad\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Remarks\n",
    "\n",
    "- The **batch size** controls the trade-off between computational cost and gradient accuracy.  \n",
    "  Small batches introduce more stochastic noise; large batches approach full GD behavior.  \n",
    "\n",
    "- The **learning rate** $\\alpha_k$ is usually constant or follows a decaying schedule, e.g.\n",
    "  \n",
    "  $$\n",
    "  \\alpha_k = \\frac{\\alpha_0}{1 + \\gamma k}, \\quad \\gamma > 0.\n",
    "  $$\n",
    "\n",
    "\n",
    "- In practice, modern optimizers (e.g. Adam, RMSProp) extend SGD with adaptive step-sizes and momentum, which we will discuss later in the course.\n",
    "\n",
    "> **Exercise (Mini-Batch Approximation):**  \n",
    "> Consider the function  \n",
    "> \n",
    "$$\n",
    "> \\mathcal{L}(\\Theta; X, Y) = \\frac{1}{N}\\sum_{i=1}^N (\\Theta^\\top x^{(i)} - y^{(i)})^2.\n",
    "> $$\n",
    "\n",
    "> 1. Implement Gradient Descent and Stochastic Gradient Descent for this loss.  \n",
    "> 2. Compare the number of iterations and the computational cost.  \n",
    "> 3. Plot the evolution of the loss across epochs for different batch sizes ($N_\\text{batch} = 1, 10, N$).  \n",
    "\n",
    "> **Exercise (Variance of the Stochastic Gradient):**  \n",
    "> For the same quadratic loss, compute the gradient over multiple random batches $\\mathcal{M}_k$ of the same size, and compare:  \n",
    "> \n",
    "$$\n",
    "> \\mathrm{Var}\\big(\\nabla_\\Theta \\mathcal{L}(\\Theta; \\mathcal{M}_k)\\big)\n",
    "> $$\n",
    "\n",
    "> as a function of the batch size.  \n",
    "> Discuss how increasing $N_\\text{batch}$ affects the variance and the convergence stability.  \n",
    "\n",
    "> **Exercise (Learning Rate Scheduling):**  \n",
    "> Implement SGD on the same dataset with three learning-rate strategies:  \n",
    "> constant $\\alpha_k=\\alpha_0$, exponentially decaying $\\alpha_k = \\alpha_0 \\, e^{-\\gamma k}$, and inverse scaling $\\alpha_k = \\frac{\\alpha_0}{1+\\gamma k}$.  \n",
    "> Compare convergence rates and discuss which choice balances speed and stability better.  \n",
    "\n",
    "## A Complete Example: Multi-Linear Regression with GD and SGD\n",
    "\n",
    "To conclude our discussion on Gradient Descent and its stochastic variant, we now consider a **complete example** where we will implement both algorithms *from scratch* (i.e., using only `numpy`) to solve a **multi-linear regression** problem on a real dataset.\n",
    "\n",
    "This exercise will help you understand how both **Gradient Descent (GD)** and **Stochastic Gradient Descent (SGD)** behave in a real training scenario and how different choices (e.g., batch size, learning rate) affect convergence.\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "We will use the **\"House Prices â€“ Advanced Regression Techniques\"** dataset from Kaggle:  \n",
    "ðŸ”— [https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)\n",
    "\n",
    "The dataset contains various features describing houses (e.g., number of rooms, area, year built) and the **sale price** as the target variable.\n",
    "\n",
    "For simplicity, we will use only a few continuous variables, for example:\n",
    "- `LotArea` (the area of the lot in square feet),\n",
    "- `OverallQual` (an overall quality rating),\n",
    "- `YearBuilt` (the construction year).\n",
    "\n",
    "Our goal is to predict the house price $ y $ using a linear model:\n",
    "\n",
    "$$\n",
    "f_\\Theta(x) = \\Theta^\\top x,\n",
    "$$\n",
    "\n",
    "where $x \\in \\mathbb{R}^d$ is the feature vector (including a bias term), and $\\Theta \\in \\mathbb{R}^d$ are the model parameters.\n",
    "\n",
    "### 1. Problem Definition\n",
    "\n",
    "We define the **Mean Squared Error (MSE)** loss function as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta; X, Y) = \\frac{1}{N}\\sum_{i=1}^N (f_\\Theta(x^{(i)}) - y^{(i)})^2\n",
    "= \\frac{1}{N}\\|X\\Theta - Y\\|_2^2,\n",
    "$$\n",
    "\n",
    "where $X$ is the data matrix of shape $(N, d)$ and $Y$ the vector of targets.\n",
    "\n",
    "The gradient of the loss with respect to $\\Theta$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\mathcal{L}(\\Theta; X, Y) = \\frac{2}{N} X^\\top (X\\Theta - Y).\n",
    "$$\n",
    "\n",
    "\n",
    "### 2. Data Preprocessing\n",
    "\n",
    "The first step is to:\n",
    "- Load the dataset (you can download the `.csv` file from Kaggle),\n",
    "- Select the numerical columns we are interested in,\n",
    "- Normalize them to have mean 0 and variance 1,\n",
    "- Add a **bias column** of ones to $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (download from Kaggle first)\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Select some useful continuous features\n",
    "features = [\"LotArea\", \"OverallQual\", \"YearBuilt\"]\n",
    "X = df[features].values\n",
    "Y = df[\"SalePrice\"].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize features\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Add bias term\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "N, d = X.shape\n",
    "print(f\"Dataset size: {N} samples, {d} features (including bias)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementing Gradient Descent\n",
    "\n",
    "We now implement Gradient Descent using the formula\n",
    "\n",
    "$$\n",
    "\\Theta^{(k+1)} = \\Theta^{(k)} - \\alpha \\, \\nabla_\\Theta \\mathcal{L}(\\Theta^{(k)}; X, Y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, Theta):\n",
    "    N = len(Y)\n",
    "    residuals = X @ Theta - Y\n",
    "    return (residuals**2).mean()\n",
    "\n",
    "def compute_grad(X, Y, Theta):\n",
    "    N = len(Y)\n",
    "    return (2/N) * X.T @ (X @ Theta - Y)\n",
    "\n",
    "def gradient_descent(X, Y, lr=1e-3, epochs=500):\n",
    "    Theta = np.zeros((X.shape[1], 1))\n",
    "    losses = []\n",
    "    for k in range(epochs):\n",
    "        grad = compute_grad(X, Y, Theta)\n",
    "        Theta -= lr * grad\n",
    "        losses.append(compute_loss(X, Y, Theta))\n",
    "    return Theta, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implementing Stochastic Gradient Descent\n",
    "\n",
    "For SGD, at each iteration we approximate the full gradient by sampling a random **mini-batch** $\\mathcal{M}_k$ of size $N_\\text{batch}$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\mathcal{L}(\\Theta; \\mathcal{M}_k) = \\frac{2}{N_\\text{batch}} X_{\\mathcal{M}_k}^\\top (X_{\\mathcal{M}_k}\\Theta - Y_{\\mathcal{M}_k}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X, Y, lr=1e-3, batch_size=64, epochs=50):\n",
    "    Theta = np.zeros((X.shape[1], 1))\n",
    "    N = len(Y)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(N)\n",
    "        for start in range(0, N, batch_size):\n",
    "            batch_idx = indices[start:start+batch_size]\n",
    "            Xb, Yb = X[batch_idx], Y[batch_idx]\n",
    "            grad = (2/len(Yb)) * Xb.T @ (Xb @ Theta - Yb)\n",
    "            Theta -= lr * grad\n",
    "        losses.append(compute_loss(X, Y, Theta))\n",
    "    return Theta, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training and Comparison\n",
    "\n",
    "We can now train both algorithms and compare their convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both GD and SGD\n",
    "Theta_gd, loss_gd = gradient_descent(X, Y, lr=1e-3, epochs=500)\n",
    "Theta_sgd, loss_sgd = sgd(X, Y, lr=1e-3, batch_size=64, epochs=50)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_gd, label=\"GD\")\n",
    "plt.plot(loss_sgd, label=\"SGD\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Convergence of GD vs. SGD\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Discussion\n",
    "\n",
    "- **Gradient Descent (GD)** computes the exact gradient at each iteration, ensuring smooth convergence, but is computationally expensive for large datasets.  \n",
    "- **SGD**, by using mini-batches, introduces stochastic noise in the updates but drastically reduces computational cost per iteration.  \n",
    "  This noise can even **help** avoid getting stuck in local minima (for non-convex problems) and improves generalization.  \n",
    "- The **learning rate** and **batch size** jointly control the trade-off between convergence speed and stability.\n",
    "\n",
    "You should experiment with:\n",
    "- Different values of `lr` (e.g., `1e-2`, `5e-4`),\n",
    "- Different batch sizes (`N_batch = 1, 32, 128, N`),\n",
    "- And normalization (try running without standardization to see instability).\n",
    "\n",
    "### 7. Questions for Reflection\n",
    "\n",
    "1. Compare the convergence curves of GD and SGD. Which is smoother? Which converges faster in terms of time per epoch?  \n",
    "2. How does batch size influence the final convergence point and oscillations?  \n",
    "3. If you replace the fixed learning rate with a decaying schedule (e.g. $\\alpha_k = \\frac{\\alpha_0}{1+k}$), how does the behavior change?  \n",
    "4. Compute the norm of the final gradient $\\|\\nabla_\\Theta \\mathcal{L}(\\Theta^{(E)}; X, Y)\\|$ for both algorithms and interpret its value.\n",
    "\n",
    "This exercise consolidates all the concepts discussed so far:\n",
    "- decomposition of the loss as a sum over samples,\n",
    "- explicit computation of gradients,\n",
    "- iterative updates via Gradient Descent and SGD,\n",
    "- and the trade-offs between accuracy, stability, and computational cost.\n",
    "\n",
    "You are now ready to apply these methods to more complex models (e.g. logistic regression or neural networks), where SGD remains the core optimization strategy.\n",
    "\n",
    "## Adam: Adaptive Moment Estimation\n",
    "\n",
    "**Adam** is an optimization algorithm that improves on standard SGD by using both  \n",
    "**momentum** (to accelerate convergence) and **adaptive learning rates** (to handle parameters with different scales).\n",
    "\n",
    "Given a stochastic gradient at iteration \\(k\\),\n",
    "\n",
    "$$\n",
    "g^{(k)} \\;=\\; \\nabla_\\Theta \\mathcal{L}\\big(\\Theta^{(k)}; \\mathcal{M}_k\\big),\n",
    "$$\n",
    "\n",
    "Adam maintains two **exponential moving averages (EMAs)**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m^{(k)} &= \\beta_1\\, m^{(k-1)} + (1-\\beta_1)\\, g^{(k)} &&\\text{(first moment: mean of gradients)}\\\\[4pt]\n",
    "v^{(k)} &= \\beta_2\\, v^{(k-1)} + (1-\\beta_2)\\, (g^{(k)})^2 &&\\text{(second moment: variance of gradients)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "After correcting the initialization bias:\n",
    "\n",
    "$$\n",
    "\\hat m^{(k)} = \\frac{m^{(k)}}{1-\\beta_1^k}, \n",
    "\\qquad \n",
    "\\hat v^{(k)} = \\frac{v^{(k)}}{1-\\beta_2^k},\n",
    "$$\n",
    "\n",
    "the parameter update is:\n",
    "\n",
    "$$\n",
    "\\Theta^{(k+1)} = \\Theta^{(k)} - \\alpha\\, \\frac{\\hat m^{(k)}}{\\sqrt{\\hat v^{(k)}} + \\varepsilon}.\n",
    "$$\n",
    "\n",
    "Typical choices:  \n",
    "\\(\\beta_1=0.9, \\; \\beta_2=0.999, \\; \\varepsilon = 10^{-8}\\).\n",
    "\n",
    "### Why Normalization Matters\n",
    "\n",
    "Before applying Adam (or any gradient-based optimizer), it is essential to **normalize or standardize the data**.\n",
    "\n",
    "1. **Balanced feature scales:**  \n",
    "   Features measured in different units (e.g., square meters, years, ratings) produce gradients of very different magnitudes, making the loss landscape highly anisotropic and ill-conditioned.\n",
    "\n",
    "2. **Stable updates:**  \n",
    "   Adam adapts learning rates *per parameter* based on the running averages of gradients and squared gradients. If features or target values have very different scales, the computed \\(v^{(k)}\\) can vary by orders of magnitude across dimensions, leading to unstable or diverging updates.\n",
    "\n",
    "3. **Faster and smoother convergence:**  \n",
    "   Centering and scaling features makes the contours of the loss more spherical, helping gradient-based methods move more directly toward the minimum.\n",
    "\n",
    "Hence, we always **standardize both the features and the target** in regression tasks:\n",
    "\n",
    "$$\n",
    "x_j \\leftarrow \\frac{x_j - \\mu_j}{\\sigma_j}, \n",
    "\\quad\n",
    "y \\leftarrow \\frac{y - \\mu_y}{\\sigma_y}.\n",
    "$$\n",
    "\n",
    "After training, predictions can be **denormalized** back to the original scale.\n",
    "\n",
    "### Implementing Adam with Standardized Data\n",
    "\n",
    "We reuse the Kaggle *House Prices* dataset but now **normalize both inputs and outputs** before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Select features\n",
    "features = [\"LotArea\", \"OverallQual\", \"YearBuilt\"]\n",
    "X = df[features].values\n",
    "Y = df[\"SalePrice\"].values.reshape(-1, 1)\n",
    "\n",
    "# === STANDARDIZATION ===\n",
    "X_mean, X_std = X.mean(axis=0), X.std(axis=0)\n",
    "Y_mean, Y_std = Y.mean(), Y.std()\n",
    "\n",
    "X = (X - X_mean) / X_std\n",
    "Y = (Y - Y_mean) / Y_std\n",
    "\n",
    "# Add bias column\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "N, d = X.shape\n",
    "\n",
    "def compute_loss(X, Y, Theta):\n",
    "    residuals = X @ Theta - Y\n",
    "    return (residuals**2).mean()\n",
    "\n",
    "def compute_grad(X, Y, Theta):\n",
    "    N = len(Y)\n",
    "    return (2/N) * X.T @ (X @ Theta - Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer (with mini-batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(\n",
    "    X, Y, \n",
    "    lr=1e-4,                # smaller LR for stability\n",
    "    beta1=0.9, beta2=0.999, eps=1e-8, \n",
    "    batch_size=128, \n",
    "    epochs=100,\n",
    "    theta0=None,\n",
    "    shuffle=True\n",
    "):\n",
    "    N, d = X.shape\n",
    "    Theta = np.zeros((d, 1)) if theta0 is None else theta0.copy()\n",
    "\n",
    "    m = np.zeros_like(Theta)\n",
    "    v = np.zeros_like(Theta)\n",
    "    t = 0\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if shuffle:\n",
    "            idx = np.random.permutation(N)\n",
    "        else:\n",
    "            idx = np.arange(N)\n",
    "\n",
    "        for start in range(0, N, batch_size):\n",
    "            batch_idx = idx[start:start+batch_size]\n",
    "            Xb, Yb = X[batch_idx], Y[batch_idx]\n",
    "            g = (2/len(Yb)) * Xb.T @ (Xb @ Theta - Yb)\n",
    "\n",
    "            # Adam updates\n",
    "            t += 1\n",
    "            m = beta1 * m + (1 - beta1) * g\n",
    "            v = beta2 * v + (1 - beta2) * (g * g)\n",
    "            m_hat = m / (1 - beta1**t)\n",
    "            v_hat = v / (1 - beta2**t)\n",
    "            Theta -= lr * (m_hat / (np.sqrt(v_hat) + eps))\n",
    "\n",
    "        losses.append(compute_loss(X, Y, Theta))\n",
    "\n",
    "    return Theta, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: SGD vs. Adam (with standardized data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X, Y, lr=1e-3, batch_size=64, epochs=100):\n",
    "    Theta = np.zeros((X.shape[1], 1))\n",
    "    losses = []\n",
    "    N = len(Y)\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.permutation(N)\n",
    "        for start in range(0, N, batch_size):\n",
    "            batch_idx = idx[start:start+batch_size]\n",
    "            Xb, Yb = X[batch_idx], Y[batch_idx]\n",
    "            grad = (2/len(Yb)) * Xb.T @ (Xb @ Theta - Yb)\n",
    "            Theta -= lr * grad\n",
    "        losses.append(compute_loss(X, Y, Theta))\n",
    "    return Theta, losses\n",
    "\n",
    "# Train both\n",
    "Theta_sgd, loss_sgd = sgd(X, Y, lr=1e-3, batch_size=64, epochs=100)\n",
    "Theta_adam, loss_adam = adam(X, Y, lr=1e-3, batch_size=64, epochs=100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_sgd, label=\"SGD\")\n",
    "plt.plot(loss_adam, label=\"Adam\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Normalized MSE Loss\")\n",
    "plt.title(\"SGD vs. Adam on Standardized Data\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "After standardization:\n",
    "\n",
    "- Both **SGD** and **Adam** converge smoothly and to nearly identical minima.  \n",
    "- **Adam** usually shows faster initial loss reduction due to its adaptive scaling of gradients.  \n",
    "- Without normalization, the gradient magnitudes differ across parameters, causing Adamâ€™s per-parameter learning rates to explode or vanish.  \n",
    "- In normalized space, the curvature of the loss is more uniform, allowing adaptive methods like Adam to perform as intended.\n",
    "\n",
    "Finally, when producing predictions in the original scale, remember to **denormalize** the output:\n",
    "\n",
    "$$\n",
    "\\hat{y}_\\text{real} = \\hat{y}_\\text{normalized} \\cdot \\sigma_Y + \\mu_Y.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "SGD has some drawbacks compared to GD. In particular, there is no way to check whether it reached the convergence (since we can't obviously compute the gradient of $\\ell(\\theta; X, Y)$ to check its distance from zero, as it is required for the first Stopping Criteria) and we can't use the backtracking algorithm, for the same reason. As a consequence, the algorithm will stop ONLY after reaching the fixed number of epochs, and we must set a good value for the step size $\\alpha_k$ by hand. Those problems are partially solved by recent algorithms like SGD with Momentum, Adam, AdaGrad, ... whose study is beyond the scope of the course.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
