
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Introduction to Optimization &#8212; Statistical and Mathematical Methods for Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Optimization/gradient_descent2024';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Statistical and Mathematical Methods for Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Statistical and Mathematical Methods for Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Statistical and Mathematical Methods for Machine Learning (SMM)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">NLA with Python</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../NLA_numpy/basics_python.html">Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../NLA_numpy/introduction_to_numpy.html">Introduction to Python for NLA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../NLA_numpy/matplotlib.html">Visualization with Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../NLA_numpy/linear_systems.html">Solving Linear Systems with Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Basics of Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../ML/intro_ML.html">A (very short) introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ML/SVD.html">Data Compression with Singular Value Decomposition (SVD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ML/PCA.html">Dimensionality Reduction with PCA</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="GD.html">Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="SGD.html">Stochastic Gradient Descent</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Homeworks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Homeworks/HW1.html">HW 1: Linear Algebra and Floating Point Arithmetic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Homeworks/HW2.html">HW 2: SVD and PCA for Machine Learning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/devangelista2/statistical-mathematical-methods" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/devangelista2/statistical-mathematical-methods/issues/new?title=Issue%20on%20page%20%2FOptimization/gradient_descent2024.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Optimization/gradient_descent2024.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction to Optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Introduction to Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity">Convexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#first-order-conditions">First-order conditions</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-gd">Gradient descent (GD)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choice-the-initial-iterate">Choice the initial iterate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-size">Step Size</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backtracking">Backtracking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stopping-criteria">Stopping Criteria</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="introduction-to-optimization">
<h1>Introduction to Optimization<a class="headerlink" href="#introduction-to-optimization" title="Link to this heading">#</a></h1>
<p>Consider a general <em>continuous</em> function $<span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span><span class="math notranslate nohighlight">\(. Given a set of _admissible solutions_ \)</span><span class="math notranslate nohighlight">\(\Omega \subseteq \mathbb{R}^n\)</span>$, an optimization problem is a problem of the form</p>
<div class="math notranslate nohighlight">
\[
    \min_{x \in \Omega} f(x)
\]</div>
<p>If $<span class="math notranslate nohighlight">\(\Omega = \mathbb{R}^n\)</span><span class="math notranslate nohighlight">\(, we say that the optimization is **unconstrained**. If \)</span><span class="math notranslate nohighlight">\(\Omega \subset \mathbb{R}^n\)</span><span class="math notranslate nohighlight">\(, the problem is **constrained**. In this first part, we will always assume \)</span><span class="math notranslate nohighlight">\(\Omega = \mathbb{R}^n\)</span>$, i.e. the unconstrained setup.</p>
<section id="convexity">
<h2>Convexity<a class="headerlink" href="#convexity" title="Link to this heading">#</a></h2>
<p>A common assumption in optimization is the <em>convexity</em>. By definition, a function $<span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span>$ is convex if</p>
<div class="math notranslate nohighlight">
\[
    f(tx_1 + (1-t)x_2) \leq tf(x_1) + (1-t)f(x_2) \qquad \forall x_1, x_2 \in \mathbb{R}^n, \quad \forall t \in [0, 1]
\]</div>
<p>{% include figure.html path=”assets/images/GD/convexity.png” title=”diagram” class=”img-fluid rounded z-depth-1” %}</p>
<p>The importance of convex functions in optimization is that if $<span class="math notranslate nohighlight">\(f(x)\)</span><span class="math notranslate nohighlight">\( is convex, than every minimum point \)</span><span class="math notranslate nohighlight">\(x^*\)</span><span class="math notranslate nohighlight">\( of \)</span><span class="math notranslate nohighlight">\(f(x)\)</span><span class="math notranslate nohighlight">\( is a _global minimum_ and the set of global minima is connected (the \)</span><span class="math notranslate nohighlight">\(n\)</span>$-dimensional extension of the concept of <em>interval</em>). On the other side, if a function is non-convex (<em>NOTE:</em> the opposite of convex is <strong>not</strong> concave), then there can be multiple distinct minimum points, some of which are <em>local minimum</em> while others are <em>global minimum</em>. Since in ML applications we want to find global minima, and discriminating between local and global minima of a function is an NP-hard problem, having convexity is a good thing.</p>
<p>{% include figure.html path=”assets/images/GD/non_convexity.png” title=”diagram” class=”img-fluid rounded z-depth-1” %}</p>
</section>
<section id="first-order-conditions">
<h2>First-order conditions<a class="headerlink" href="#first-order-conditions" title="Link to this heading">#</a></h2>
<p>Most of the algorithms to find the minimum points of a given function $<span class="math notranslate nohighlight">\(f(x)\)</span>$ are based on the following property:</p>
<blockquote>
<div><p><strong><em>First-order sufficient condition:</em></strong> If $<span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span><span class="math notranslate nohighlight">\( is a continuously differentiable function and \)</span><span class="math notranslate nohighlight">\(x^* \in \mathbb{R}^n\)</span><span class="math notranslate nohighlight">\( is a minimum point of \)</span><span class="math notranslate nohighlight">\(f(x)\)</span><span class="math notranslate nohighlight">\(, then \)</span><span class="math notranslate nohighlight">\(\nabla f(x^*) = 0\)</span>$.</p>
</div></blockquote>
<p>Moreover, it holds</p>
<blockquote>
<div><p><strong><em>First-order necessary condition:</em></strong> If $<span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span><span class="math notranslate nohighlight">\( is a continuously differentiable function and \)</span><span class="math notranslate nohighlight">\(\nabla f( x^* ) = 0\)</span><span class="math notranslate nohighlight">\( for \)</span><span class="math notranslate nohighlight">\(x^* \in \mathbb{R}^n\)</span><span class="math notranslate nohighlight">\(, then \)</span><span class="math notranslate nohighlight">\(x^*\)</span><span class="math notranslate nohighlight">\( is either a (local) minimum, a (local) maximum or a saddle point of \)</span><span class="math notranslate nohighlight">\(f(x)\)</span>$.</p>
</div></blockquote>
<p>Consequently, we want to find a point $<span class="math notranslate nohighlight">\(x^* \in \mathbb{R}^n\)</span><span class="math notranslate nohighlight">\( such that \)</span><span class="math notranslate nohighlight">\(\nabla f(x^*) = 0\)</span><span class="math notranslate nohighlight">\(. Those points are called **stationary points** of \)</span><span class="math notranslate nohighlight">\(f(x)\)</span>$.</p>
<p>{% include figure.html path=”assets/images/GD/minmaxsaddle.png” title=”diagram” class=”img-fluid rounded z-depth-1” %}</p>
</section>
</section>
<section id="gradient-descent-gd">
<h1>Gradient descent (GD)<a class="headerlink" href="#gradient-descent-gd" title="Link to this heading">#</a></h1>
<p>The most common algorithm to solve optimization problems is the so-called Gradient Descent (GD). It is an iterative algorithm, i.e. an algorithm that iteratively updates the estimate of the solution, converging to the correct solution after infinite steps, such that, at each iteration, the successive estimate is computed by moving in the direction of maximum decreasing of $<span class="math notranslate nohighlight">\(f(x)\)</span><span class="math notranslate nohighlight">\(: \)</span><span class="math notranslate nohighlight">\(- \nabla f(x)\)</span>$. Specifically,</p>
<div class="math notranslate nohighlight">
\[
    x_{k+1} = x_k - \alpha_k \nabla f(x_k) \qquad k = 0, 1, \dots
\]</div>
<p>where the initial iterate, $<span class="math notranslate nohighlight">\(x_0 \in \mathbb{R}^n\)</span><span class="math notranslate nohighlight">\(, is given as input and the **step-size** (equivalently, **learning rate**) \)</span><span class="math notranslate nohighlight">\(\alpha_k &gt; 0\)</span><span class="math notranslate nohighlight">\( controls the decay rapidity of \)</span><span class="math notranslate nohighlight">\(f(x)\)</span><span class="math notranslate nohighlight">\( for any \)</span><span class="math notranslate nohighlight">\(k \in \mathbb{N}\)</span>$.</p>
<section id="choice-the-initial-iterate">
<h2>Choice the initial iterate<a class="headerlink" href="#choice-the-initial-iterate" title="Link to this heading">#</a></h2>
<p>The Gradient Descent (GD) algorithm, always require the user to input an initial iterate $<span class="math notranslate nohighlight">\(x_0 \in \mathbb{R}^n\)</span><span class="math notranslate nohighlight">\(. Theoretically, since GD has a _global convergence_ proprerty, for any \)</span><span class="math notranslate nohighlight">\(x_0\)</span><span class="math notranslate nohighlight">\( it will always converge to a **stationary point** of \)</span><span class="math notranslate nohighlight">\(f(x)\)</span><span class="math notranslate nohighlight">\(, i.e. to a point such that \)</span><span class="math notranslate nohighlight">\(\nabla f(x) = 0\)</span>$.</p>
<p>If $<span class="math notranslate nohighlight">\(f(x)\)</span><span class="math notranslate nohighlight">\( is convex, then every stationary point is a (global) minimum of \)</span><span class="math notranslate nohighlight">\(f(x)\)</span><span class="math notranslate nohighlight">\(, implying that the choice of \)</span><span class="math notranslate nohighlight">\(x_0\)</span><span class="math notranslate nohighlight">\( is not really important, and we can always use \)</span><span class="math notranslate nohighlight">\(x_0 = 0\)</span><span class="math notranslate nohighlight">\(. On the other side, when \)</span><span class="math notranslate nohighlight">\(f(x)\)</span><span class="math notranslate nohighlight">\( is not convex, we have to choose \)</span><span class="math notranslate nohighlight">\(x_0\)</span><span class="math notranslate nohighlight">\( such that it is as close as possible to the _right_ stationary point, to increase the chances of getting to that. If an estimate of the correct minimum point is not available, we will just consider \)</span><span class="math notranslate nohighlight">\(x_0 = 0\)</span>$ to get to a general local minima.</p>
</section>
<section id="step-size">
<h2>Step Size<a class="headerlink" href="#step-size" title="Link to this heading">#</a></h2>
<p>Choosing the step size is the hardest component of gradient descent algorithm. Indeed, if $<span class="math notranslate nohighlight">\(\alpha_k\)</span><span class="math notranslate nohighlight">\( is too small, there is a chance we never get to the minimum, getting closer and closer without reaching it. Moreover, we can easily get stuck on local minima when the _objective function_ is non convex. On the contrary, if \)</span><span class="math notranslate nohighlight">\(\alpha_k\)</span>$ is too large, there is a chance we get stuck, bouncing back and forth around the minima.</p>
<p>{% include figure.html path=”assets/images/GD/right_alpha.png” title=”diagram” class=”img-fluid rounded z-depth-1” %}</p>
</section>
<section id="backtracking">
<h2>Backtracking<a class="headerlink" href="#backtracking" title="Link to this heading">#</a></h2>
<p>Choosing the right step-size at each iteration is non-trivial. Indeed, convergence of Gradient Descent methods is only guaranteed if the step-size $<span class="math notranslate nohighlight">\(\alpha_k\)</span><span class="math notranslate nohighlight">\( satisfies, for any \)</span><span class="math notranslate nohighlight">\(k \in \mathbb{N}\)</span>$, some conditions known as <em>Wolfe Conditions</em>:</p>
<ul class="simple">
<li><p>Sufficient decrease: $<span class="math notranslate nohighlight">\( f(x_k - \alpha_k \nabla f(x_k)) \leq f(x_k) - c_1 \alpha_k \| \nabla f(x_k) \|_2^2 \)</span>$;</p></li>
<li><p>Curvature condition: $<span class="math notranslate nohighlight">\(\nabla f(x_k)^T \nabla f(x_k - \alpha_k \nabla f(x_k)) \leq c_2 \|  \nabla f(x_k)  \|_2^2\)</span>$;</p></li>
</ul>
<p>with $<span class="math notranslate nohighlight">\(0 &lt; c_1 &lt; c_2 &lt; 1\)</span>$.</p>
<p>Luckily, those conditions are automatically satisfied if $<span class="math notranslate nohighlight">\(\alpha_k\)</span><span class="math notranslate nohighlight">\( is chosen by the **backtracking algorithm**. The idea of this algorithm is to start from an initial guess for \)</span><span class="math notranslate nohighlight">\(\alpha_k\)</span><span class="math notranslate nohighlight">\(, and then reducing it as \)</span><span class="math notranslate nohighlight">\(\alpha_k \leftarrow \tau \alpha_k\)</span><span class="math notranslate nohighlight">\( with \)</span><span class="math notranslate nohighlight">\(\tau &lt; 1\)</span>$ until the sufficient decrease condition is satisfied. A Python implementation for the backtracking algorithm can be found in the following.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">backtracking</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad_f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function is a simple implementation of the backtracking algorithm for</span>
<span class="sd">    the GD (Gradient Descent) method.</span>
<span class="sd">    </span>
<span class="sd">    f: function. The function that we want to optimize.</span>
<span class="sd">    grad_f: function. The gradient of f(x).</span>
<span class="sd">    x: ndarray. The actual iterate x_k.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">c</span> <span class="o">=</span> <span class="mf">0.8</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="mf">0.25</span>
    
    <span class="k">while</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">&gt;</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">tau</span> <span class="o">*</span> <span class="n">alpha</span>
        
        <span class="k">if</span> <span class="n">alpha</span> <span class="o">&lt;</span> <span class="mf">1e-3</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">alpha</span>
</pre></div>
</div>
</section>
<section id="stopping-criteria">
<h2>Stopping Criteria<a class="headerlink" href="#stopping-criteria" title="Link to this heading">#</a></h2>
<p>The gradient descent is an iterative algorithm, meaning that it iteratively generates new estimates of the minima, starting from <span class="math notranslate nohighlight">\(x_0\)</span>. Theoretically, after infinite iterations, we converge to the solution of the optimization problem but, since we cannot run infinite iterations, we have to find a way to tell the algorithm when its time to stop. A convergence condition for an iterative algorithm is called <strong>stopping criteria</strong>.</p>
<p>Remember that gradient descent aim to find stationary point. Consequently, it would make sense to use the norm of the gradient as a stopping criteria. In particular, it is common to check if the norm of the gradient on the actual iterate is below a certain tollerance and, if so, we stop the iterations. In particular</p>
<blockquote>
<div><p><strong><em>Stopping criteria 1:</em></strong> Given a tollerance $<span class="math notranslate nohighlight">\(tol_f\)</span><span class="math notranslate nohighlight">\(, for any iterate \)</span><span class="math notranslate nohighlight">\(x_k\)</span><span class="math notranslate nohighlight">\(, check whether or not \)</span><span class="math notranslate nohighlight">\(\|\| \nabla f(x_k) \|\| &lt; tol_f \|\| \nabla f(x_0) \|\|\)</span>$. If so, stop the iterations.</p>
</div></blockquote>
<p>Unfortunately, this condition alone is not sufficient. Indeed, if the function $<span class="math notranslate nohighlight">\(f(x)\)</span><span class="math notranslate nohighlight">\( is almost flat around its minimum, then \)</span><span class="math notranslate nohighlight">\(\|\| \nabla f(x_k) \|\|\)</span><span class="math notranslate nohighlight">\( will be small even if \)</span><span class="math notranslate nohighlight">\(x_k\)</span>$ will be far from the true minimum.</p>
<p>{% include figure.html path=”assets/images/GD/plateau.png” title=”diagram” class=”img-fluid rounded z-depth-1” %}</p>
<p>Consequently, its required to add another stopping criteria.</p>
<blockquote>
<div><p><strong><em>Stopping criteria 2:</em></strong> Given a tollerance $<span class="math notranslate nohighlight">\(tol_x\)</span><span class="math notranslate nohighlight">\(, for any iterate \)</span><span class="math notranslate nohighlight">\(x_k\)</span><span class="math notranslate nohighlight">\(, check whether or not \)</span><span class="math notranslate nohighlight">\(\|\| x_k - x_{k-1} \|\| &lt; tol_x \|\| x_0 \|\|\)</span>$. If so, stop the iterations.</p>
</div></blockquote>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Optimization"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Introduction to Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity">Convexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#first-order-conditions">First-order conditions</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-gd">Gradient descent (GD)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choice-the-initial-iterate">Choice the initial iterate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-size">Step Size</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backtracking">Backtracking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stopping-criteria">Stopping Criteria</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Davide Evangelista
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>