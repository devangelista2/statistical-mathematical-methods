{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: SVD and PCA for Machine Learning\n",
    "\n",
    "```{warning}\n",
    "The submission of the homeworks has **NO** deadline. You can submit them whenever you want, on Virtuale. You are only required to upload it on Virtuale **BEFORE** your exam session, since the Homeworks will be a central part of the oral exam. \n",
    "\n",
    "You are asked to submit the homework as one of the two, following modalities:\n",
    "* A PDF (or Word) document, containing screenshoots of code snippets, screeshots of the results generated by your code, and a brief comment on the obtained results.\n",
    "* A Python Notebook (i.e. a `.ipynb` file), with cells containing the code required to solve the indicated exercises, alternated with a brief comment on the obtained results in the form of a markdown cell. We remark that the code **SHOULD NOT** be runned during the exam, but the student is asked to enter the exam with all the programs **already executed**, with the results clearly visible on the screen.\n",
    "\n",
    "Joining the oral exam with a non-executed code OR without a PDF file with the obtained results visible on that, will cause the student to be rejected.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing dyad\n",
    "Consider an image from `skimage.data`. For simplicity, say that $X \\in \\mathbb{R}^{m \\times n}$ is the matrix representing that image. You are asked to visualize the dyad of the SVD Decomposition of $X$ and the result of compressing the image via SVD. In particular:\n",
    "\n",
    "* Load the image into memory and compute its SVD;\n",
    "* Visualize some of the dyad $\\sigma_i u_i v_i^T$ of this decomposition. What do you notice?\n",
    "* Plot the singular values of $X$. Do you note something?\n",
    "* Visualize the $k$-rank approximation of $X$ for different values of $k$. What do you observe?\n",
    "* Compute and plot the approximation error $|| X − X_k ||_F$ for increasing values of $k$, where $X_k$ is the $k$-rank approximation of $k$.\n",
    "* Plot the compression factor: $c_k = 1 − \\frac{k(m+n+1)}{mn}$ for increasing $k$.\n",
    "* Compute the value $k$ such that $c_k = 0$ (i.e. when the compressed image requires the same amount of informations of those of the uncompressed image). What is the approximation error for this value of $k$? Comment.\n",
    "\n",
    "It is strongly recommended (but not mandatory) to consider a grey-scale image for this exercise. You can also use an image downloaded from the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of MNIST Digits with SVD Decomposition.\n",
    "The task for this exercise is to learn the classification of MNIST digits by using SVD decomposition.\n",
    "To proceed we recall that, given a matrix $X \\in \\mathbb{R}^{d \\times N}$ and its SVD decomposition $X = USV^T$, it is easy to show that an orthogonal basis for the space of the columns is given by the first $p$ columns of the matrix $U$, where $p = rank(X)$ is equal to the number of non-zero singular values of $X$. We will make use of the space of the columns defined by the $U$ matrix and the following Theorem:\n",
    "\n",
    "**Theorem 1.** Let $W$ be a subspace of $\\mathbb{R}^d$ with $dim W = s$, and let ${w_1, \\dots, w_s}$ be an orthogonal basis of $W$. Then, for any $x \\in \\mathbb{R}^d$, the projection $x^\\perp$ of $x$ onto $W$ has the following form:\n",
    "\n",
    "$$\n",
    "x^\\perp = \\frac{x \\cdot w_1}{w_1 \\cdot w_1} w_1 + \\dots + \\frac{x \\cdot w_s}{w_s \\cdot w_s} w_s.\n",
    "$$\n",
    "\n",
    "**Corollary 1.1.** Let $X \\in \\mathbb{R}^{d \\times N}$ be a matrix with SVD decomposition $X = USV^T$, since $p = rank(X)$ is the dimension of the space defined by the columns of $X$ and the columns of $U$, ${u_1, \\dots, u_p}$ are an orthonormal basis for that space, the projection of an $d$-dimensional vector $x$ on this space can be easily\n",
    "computed as:\n",
    "\n",
    "$$\n",
    "x^\\perp = U(U^T x).\n",
    "$$\n",
    "\n",
    "Now, consider a binary classification problem, where the task is to tell if a given hand-written digit represents the number 3 or the number 4. We will refer to the class of the number 3 as $C_1$, and to the class of the number 4 as $C_2$. Let $N_1$ be the number of elements in $C_1$, and $N_2$ be the number of elements in $C_2$. Let $X_1 \\in \\mathbb{R}^{d \\times N_1}$ be the matrix such that its columns are a flatten version of each digit in $C_1$, $X_2 \\in \\mathbb{R}^{d \\times N_2}$ be the matrix such that its columns are a flatten version of each digit in $C_2$, and consider:\n",
    "\n",
    "$$\n",
    "X_1 = U_1S_1V_1^T, \\\\\n",
    "X_2 = U_2S_2V_2^T,\n",
    "$$\n",
    "\n",
    "the SVD decomposition of the two matrices.\n",
    "\n",
    "If $x \\in \\mathbb{R}^{d}$ is a new, unknown digit, we can classify it by projecting it to the spaces of $X_1$ and $X_2$ and call them:\n",
    "\n",
    "$$\n",
    "x_1^\\perp = U_1(U_1^T x), \\\\\n",
    "x_2^\\perp = U_2(U_2^T x).\n",
    "$$\n",
    "\n",
    "Then, we can classify $x$ as $C_1$ if $||x − x_1^\\perp ||_2 < ||x−x_2^\\perp ||_2$ and vice versa as $C_2$ if $||x−x_2^\\perp||_2 < || x − x_1^\\perp ||_2$. In this exercise, you are required to implement this idea in Python.\n",
    "\n",
    "1. Implement the binary classification algorithm discussed above for the digits 3 and 4 of MNIST dataset. Follow these steps:\n",
    "   * Download the MNIST dataset from [kaggle.com/datasets/animatronbot/mnist-digit-recognizer](https://www.kaggle.com/datasets/animatronbot/mnist-digit-recognizer) and load it into memory by following the steps we did in the [PCA class](https://devangelista2.github.io/statistical-mathematical-methods/ML/PCA.html). This dataset is presented as an array with shape $42000 \\times 785$ , containining the flattened version of $42000$ $28 \\times 28$ grayscale handwritten digits, plus a column representing the true class of the corresponding digit. By pre-processing the data as we did in class, you should obtain a matrix `X` containing the flattenened digits, with shape `(784, 42000)`, and a vector `Y` of the associated digit value, with a shape of `(42000,)`.\n",
    "   * Write a function taking as input an index value `idx` and visualizes the image of `X` in the corresponding index (i.e. `X[idx, :]`). Use the function `plt.imshow`.\n",
    "   * Extract from `X` those elements that corresponds to digits 3 or 4. Those digits represents the classes $C_1$ and $C_2$ defined above.\n",
    "   * Split the obtained dataset in training and testing in a proportion of $80 : 20$. From now on, we will only consider the training set. The test set will be only used at the end of the exercise to test the algorithm.\n",
    "   * Call `X1` and `X2` the submatrices of the training set, filtered by the two selected digits.\n",
    "   * Compute the SVD decomposition of `X1` and `X2` with `np.linalg.svd(matrix, full matrices=False)` and denote the $U$-part of the two decompositions as `U1` and `U2`.\n",
    "   * Take an unknown digit $x$ from the test set, and compute $x_1^\\perp = U_1(U_1^T x)$ and $x_2^\\perp = U_2(U_2^T x)$.\n",
    "   * Compute the distances $d_1 = || x − x_1^\\perp ||_2$ and $d_2 = || x − x_2^\\perp||_2$, and classify $x$ as $C_1$ if $d_1 < d_2$, as $C_2$ if $d_2 < d_1$.\n",
    "   * Repeat the experiment for different values of $x$ in the test set. Compute the misclassification number for this algorithm.\n",
    "   * Repeat the experiment for different digits other than 3 or 4. There is a relationship between the visual similarity of the digits and the classification error?\n",
    "   * Comment the obtained results.\n",
    "\n",
    "2. The extension of this idea to the multiple classification task is trivial. Indeed, if we have more than 2 classes (say, $k$ different classes) $C_1, \\dots, C_k$, we just need to repeat the same procedure as before for each matrix $X_1, \\dots, X_k$ to obtain the distances $d_1, \\dots, d_k$. Then, the new digit $y$ will be classified as $C_i$ if $d_i$ is lower that $d_j$ for each $j = 1,...,k$. Repeat the exercise above with a 3-digit example. Comment the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with PCA\n",
    "The task for this exercise is to verify the ability of PCA in clustering data by projecting very high-dimensional datapoints to 2 or 3 dimensions. In particular, consider the same MNIST dataset used in the previous exercise. You are asked to:\n",
    "* Load and pre-process the dataset as did in the previous exercise, to get the matrix `X` with shape `(784, 42000)`, and the associated vector `Y`.\n",
    "* Choose a number of digits (for example, 0, 6 and 9) and extract from `X` and `Y` the sub-dataset containing only the considered digits.\n",
    "* Set $N_{train} < N$ and randomly sample a training set with $N_{train}$ datapoints from  `X` and `Y`. Call them `X_train` and `Y_train`. Everything else is the test set. Call them `X_test` and `Y_test`, correspondingly.\n",
    "* Implement the algorithms computing the PCA of `X_train` with a fixed $k$. Visualize the results (for $k = 2$) and the position of the centroid of each cluster;\n",
    "* Compute, for each cluster, the average distance from the centroid. Comment the result;\n",
    "* Compute, for each cluster, the average distance from the centroid on the test set. Comment the results;\n",
    "* Define a classification algorithm in this way: given a new observation `x`, compute the distance between `x` and each cluster centroid. Assign `x` to the class corresponding the the closer centroid. Compute the accuracy of this algorithm on the test set and compute its accuracy;\n",
    "* Repeat this experiment for different values of k and different digits. What do you observe?\n",
    "* Compare this classification algorithm with the one defined in the previous exercise. Which performs better?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}