{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: SVD and PCA for Machine Learning\n",
    "\n",
    "```{warning}\n",
    "The submission of the homeworks has **NO** deadline. You can submit them whenever you want, on Virtuale. You are only required to upload it on Virtuale **BEFORE** your exam session, since the Homeworks will be a central part of the oral exam. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing dyad\n",
    "Consider an image from `skimage.data`. For simplicity, say that $X \\in \\mathbb{R}^{m \\times n}$ is the matrix representing that image. You are asked to visualize the dyad of the SVD Decomposition of $X$ and the result of compressing the image via SVD. In particular:\n",
    "\n",
    "* Load the image into memory and compute its SVD;\n",
    "* Visualize some of the dyad $\\sigma_i u_i v_i^T$ of this decomposition. What do you notice?\n",
    "* Plot the singular values of $X$. Do you note something?\n",
    "* Visualize the $k$-rank approximation of $X$ for different values of $k$. What do you observe?\n",
    "* Compute and plot the approximation error $|| X − X_k ||_F$ for increasing values of $k$, where $X_k$ is the $k$-rank approximation of $k$.\n",
    "* Plot the compression factor: $c_k = 1 − \\frac{k(m+n+1)}{mn}$ for increasing $k$. What is the approximation error when the compressed image requires the same amount of informations of those of the uncompressed image (i.e. $c_k = 0$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of MNIST Digits with SVD Decomposition.\n",
    "The task for this exercise is to learn the classification of MNIST digits by using SVD decomposition.\n",
    "Remember that, Given a matrix $X \\in \\mathbb{R}^{m \\times n}$ and its SVD decomposition $X = USV^T$, we can prove that an orthogonal base for the space of the columns is given by the first $p$ columns of the matrix $U$, where $p = rank(X)$ is equal to the number of non-zero singular values of $A$. We will make use of the space of the columns defined by the $U$ matrix and the following Theorem:\n",
    "\n",
    "**Theorem 1.** Let’s consider $W$ a subspace of $\\mathbb{R}^n$ where $dim W = s$ and ${w_1, \\dots, w_s}$ an orthogonal base of $W$. Given a generic $y \\in \\mathbb{R}^n$, we have that the projection $y^\\perp$ of $y$ onto $W$ has the following form:\n",
    "\n",
    "$$\n",
    "y^\\perp = \\frac{y \\cdot w_1}{w_1 \\cdot w_1} w_1 + \\dots + \\frac{y \\cdot w_s}{w_s \\cdot w_s} w_s.\n",
    "$$\n",
    "\n",
    "**Corollary 1.1.** If $X \\in \\mathbb{R}^{m \\times n}$ is a given matrix with SVD decomposition $X = USV^T$, since $p = rank(X)$ is the dimension of the space defined by the columns of $X$ and the columns of $U$, ${u_1, \\dots, u_p}$ are an orthonormal basis for that space, the projection of an $n$-dimensional vector $y$ on this space can be easily\n",
    "computed as:\n",
    "\n",
    "$$\n",
    "y^\\perp = U(U^T y)\n",
    "$$\n",
    "\n",
    "Thus, consider a binary classification problem, where we want to classificate if a given digit of dimension $m \\times n$ represents the number 3 or the number 4. We will call refer to the class of the number 3 as $C_1$, and to the class of the number 4 as $C_2$. Suppose that $s_1$ is the number of elements in $C_1$, while $s_2$ is the number of elements in $C_2$. If $X_1 \\in \\mathbb{R}^{mn \\times s_1}$ is the matrix such that its columns are a flatten version of each digit in $C_1$, $X_2 \\in \\mathbb{R}^{mn \\times s_2}$ is the matrix such that its columns are a flatten version of each digit in $C_2$, and consider:\n",
    "\n",
    "$$\n",
    "X_1 = U_1S_1V_1^T \\\\\n",
    "X_2 = U_2S_2V_2^T\n",
    "$$\n",
    "\n",
    "the SVD decomposition of the two matrices.\n",
    "\n",
    "If $y$ in $\\mathbb{R}^{m \\times n}$ is a new, unknown digit, we can classify it by first flatten it to a vector of $\\mathbb{R}^{mn}$, then we can project it to the spaces of $X_1$ and $X_2$ and call them\n",
    "\n",
    "$$\n",
    "y_1^\\perp = U_1(U_1^T y) \\\\\n",
    "y_2^\\perp = U_2(U_2^T y)\n",
    "$$\n",
    "\n",
    "Then, $y$ will be classified as $C_1$ if $||y − y_1^\\perp ||_2 < ||y−y_2^\\perp ||_2$ and vice versa will be classified as $C_2$ if $||y−y_2^\\perp||_2 < || y − y_1^\\perp ||_2$. We want to implement this idea on Python.\n",
    "\n",
    "1. In the first exercise, we will implement the binary classification algorithm for the digits 3 and 4 of MNIST following the ideas explained above.\n",
    "* Load the MNIST dataset contained in `./data/MNIST.mat` with the function `scipy.io.loadmat`. This dataset, which is loaded in the form of a $256 \\times 1707$ matrix `X`, contains the flattened version of $1707$ $16 \\times 16$ grayscale handwritten digits. Moreover, from the same file it is possible to load a vector `I` of length $1707$ such that the $i$-th element of `I` is the true digit represented by the $i$-th image of `X`.\n",
    "* Visualize a bunch of datapoints of `X` with the function `plt.imshow`.\n",
    "* Extract from `X` those columns that corresponds to digits 3 or 4. Those digits represents the classes $C_1$ and $C_2$ defined above.\n",
    "* Split the obtained dataset in training and testing. From now on, we will only consider the training set. The test set will be only used at the end of the exercise to test the algorithm.\n",
    "* Create the matrices `X1` and `X2` defined above from `X`.\n",
    "* Compute the SVD decomposition of `X1` and `X2` with `np.linalg.svd(matrix, full matrices=False)` and denote the $U$-part of the two decompositions as `U1` and `U2`.\n",
    "* Take an unknown digit $y$ from the test set, and compute $y_1^\\perp = U_1(U_1^T y)$ and $y_2^\\perp = U_2(U_2^T y)$.\n",
    "* Compute the distances $d_1 = || y − y_1^\\perp ||_2$ and $d_2 = || y − y_2^\\perp||_2$, and classify $y$ as $C_1$ if $d_1 < d_2$, as $C_2$ if $d_2 < d_1$.\n",
    "* Repeat the experiment for different values of y in the test set. Compute the misclassification number for this algorithm.\n",
    "* Repeat the experiment for different digits other than 3 or 4. There is a relationship between the visual similarity of the digits and the classification error?\n",
    "* Comment the obtained results.\n",
    "\n",
    "2. The extension of this idea to the multiple classification task is trivial. Indeed, if we have more than 2 classes (say, $k$ different classes) $C_1, \\dots, C_k$, we just need to repeat the same procedure as before for each matrix $X_1, \\dots, X_k$ to obtain the distances $d_1, \\dots, d_k$. Then, the new digit $y$ will be classified as $C_i$ if $d_i$ is lower that $d_j$ for each $j = 1,...,k$. Repeat the exercise above with a 3-digit example. Comment the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with PCA\n",
    "The task for this exercise is to verify the ability of PCA in clustering data by projecting very high-dimensional datapoints to 2 or 3 dimensions. In particular, consider the dataset MNIST provided on Virtuale. This dataset contains images of handwritten digits with dimension $16 \\times 16$, together with a number from 0 to 9 representing the label. You are asked to:\n",
    "* Load the dataset in memory and explore its head and shape to understand how the informations are placed inside of it;\n",
    "* Split the dataset into the $X$ matrix of dimension $d \\times N$, with $d = 256$ being the dimension of each datum, $N$ is the number of datapoints, and $Y \\in \\mathbb{R}^N$ containing the corresponding labels;\n",
    "* Choose a number of digits (for example, 0, 6 and 9) and extract from $X$ and $Y$ the sub-dataset containing only the considered digits. Re-call $X$ and $Y$ those datasets, since the originals are not required anymore;\n",
    "* Set $N_{train} < N$ and randomly sample a training set with $N_{train}$ datapoints from $X$ (and the corresponding $Y$). Call them $X_{train}$ and $Y_{train}$. Everything else is the test set. Call it $X_{test}$ and $Y_{test}$.\n",
    "* Implement the algorithms computing the PCA of $X_{train}$ with a fixed $k$. Visualize the results (for $k = 2$) and the position of the centroid of each cluster;\n",
    "* Compute, for each cluster, the average distance from the centroid. Comment the result;\n",
    "* Compute, for each cluster, the average distance from the centroid on the test set. Comment the results;\n",
    "* Define a classification algorithm in this way: given a new observation $x$, compute the distance between $x$ and each cluster centroid. Assign $x$ to the class corresponding the the closer centroid. Compute the accuracy of this algorithm on the test set and compute its accuracy;\n",
    "* Repeat this experiment for different values of k and different digits. What do you observe?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}